{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib qt\n",
    "import copy\n",
    "import dill\n",
    "import os\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from kusanagi import utils\n",
    "from kusanagi.base import apply_controller, ExperienceDataset\n",
    "from kusanagi.ghost import control, regression\n",
    "from kusanagi.shell import cartpole, arduino\n",
    "from kusanagi.shell.cost import gaussian_kl_loss, convert_angle_dimensions\n",
    "from kusanagi.shell.experiment_utils import run_pilco_experiment, setup_mc_pilco_experiment, plot_rollout\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# np.random.seed(1337)\n",
    "np.set_printoptions(linewidth=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init params\n",
    "output_dir = utils.get_output_dir()\n",
    "sim2real_output_dir = '/localdata/juan/sim2real_results'\n",
    "\n",
    "\n",
    "params = cartpole.default_params()\n",
    "params['optimizer']['min_method'] = 'adam'\n",
    "params['optimizer']['max_evals'] = 1000\n",
    "params['learning_rate'] = 1e-4\n",
    "params['crn_dropout'] = True\n",
    "params['min_steps'] = 30\n",
    "n_samples = 100                     # number of MC samples for bayesian nn\n",
    "n_demo = 10                          # number of example trajectories\n",
    "pol_adjustment = False\n",
    "\n",
    "H = params['min_steps']\n",
    "gamma = params['discount']\n",
    "angle_dims = params['angle_dims']\n",
    "\n",
    "# initial state distribution\n",
    "p0 = params['state0_dist']\n",
    "D = p0.mean.size\n",
    "\n",
    "dyn_path = os.path.join(output_dir, 'cartpole_kl_loss/dynamics_21')\n",
    "pol_path = os.path.join(output_dir, 'cartpole_kl_loss/policy_21')\n",
    "exp_path = None #os.path.join(output_dir, 'cartpole_kl_loss/experience_29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_dyn(params, dyn_path=None, copy_params=True):\n",
    "\n",
    "    dyn_spec = dict(\n",
    "        hidden_dims=[200]*2,\n",
    "        p=True, p_input=True,\n",
    "        nonlinearities=regression.nonlinearities.rectify,\n",
    "        W_init=lasagne.init.GlorotNormal(gain='relu'),\n",
    "        dropout_class=regression.layers.DenseLogNormalDropoutLayer,\n",
    "        build_fn=regression.dropout_mlp)\n",
    "    \n",
    "    if dyn_path is not None:\n",
    "        # load dynamics model\n",
    "        source_dyn = regression.BNN(\n",
    "            filename=dyn_path, name='source_dyn', **params['dynamics_model'])\n",
    "    else:\n",
    "        # init dynamics model\n",
    "        source_dyn = regression.BNN(network_spec=dyn_spec, name='source_dyn', **params['dynamics_model'])\n",
    "        \n",
    "    if copy_params and dyn_path is not None:\n",
    "        target_dyn = regression.BNN(\n",
    "            filename=dyn_path, name='target_dyn', **params['dynamics_model'])\n",
    "    else:\n",
    "        target_dyn = regression.BNN(network_spec=dyn_spec, name='target_dyn', **params['dynamics_model'])\n",
    "\n",
    "    return source_dyn, target_dyn\n",
    "\n",
    "def init_pol(params,  pol_path=None, adjustment=False, copy_params=True):\n",
    "    pol_spec = dict(\n",
    "        hidden_dims=[200]*2,\n",
    "        p=0.1, p_input=0.0,\n",
    "        nonlinearities=regression.nonlinearities.rectify,\n",
    "        W_init=lasagne.init.GlorotNormal(gain='relu'),\n",
    "        dropout_class=regression.layers.DenseDropoutLayer,\n",
    "        build_fn=regression.dropout_mlp)\n",
    "\n",
    "    if pol_path is not None:\n",
    "        # load policy\n",
    "        source_pol = control.NNPolicy(params['dynamics_model']['odims'], filename=pol_path, **params['policy'])\n",
    "    else:\n",
    "        # init policy\n",
    "        source_pol = control.NNPolicy(\n",
    "            params['dynamics_model']['odims'], network_spec=pol_spec, heteroscedastic=False, **params['policy'])\n",
    "    if pol_adjustment:\n",
    "        # init adjustment model\n",
    "        target_pol = control.AdjustedPolicy(\n",
    "            source_pol, maxU=source_pol.maxU, angle_dims=source_pol.angle_dims,\n",
    "            adjustment_model_class=regression.BNN)\n",
    "        target_pol.adjustment_model.trained = True\n",
    "    else:\n",
    "        if copy_params and pol_path is not None:\n",
    "            target_pol = control.NNPolicy(\n",
    "                params['dynamics_model']['odims'], filename=pol_path, **params['policy'])\n",
    "        else:\n",
    "            target_pol = control.NNPolicy(\n",
    "                params['dynamics_model']['odims'], network_spec=pol_spec, heteroscedastic=False, **params['policy'])\n",
    "            \n",
    "    return source_pol, target_pol\n",
    "\n",
    "# init task cost\n",
    "task_cost = partial(cartpole.cartpole_loss, **params['cost'])\n",
    "\n",
    "# init source environment\n",
    "params['source'] = params['plant']\n",
    "params['source']['name'] = 'Cartpole_src'\n",
    "source_env = cartpole.Cartpole(**params['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect example trajectory data on sim environment\n",
    "source_pol = init_pol(params, pol_path)[0]\n",
    "if exp_path is not None:\n",
    "    source_exp = ExperienceDataset(filename=exp_path)\n",
    "else:\n",
    "    source_exp = ExperienceDataset()\n",
    "\n",
    "# init expert trajectory variables\n",
    "n_episodes = source_exp.n_episodes()\n",
    "if n_demo > n_episodes:\n",
    "    # function to execute before applying policy\n",
    "    def gTrig(state):\n",
    "        return utils.gTrig_np(state, angle_dims).flatten()\n",
    "\n",
    "    # function to execute after applying policy\n",
    "    def step_cb(state, action, cost, info, env=None):\n",
    "        env.render()\n",
    "\n",
    "    # apply controller\n",
    "    callback = partial(step_cb, env=source_env)\n",
    "\n",
    "    for i in range(n_demo-n_episodes):\n",
    "        ret = apply_controller(source_env, source_pol, H+1, gTrig, callback)\n",
    "        source_exp.append_episode(*ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source trajectory\n",
    "trajs = np.array(source_exp.states)\n",
    "tr_shape = trajs.shape\n",
    "\n",
    "trajs = utils.gTrig_np(trajs.reshape((tr_shape[0]*tr_shape[1], tr_shape[2])), angle_dims)\n",
    "trajectories = trajs.reshape((tr_shape[0], tr_shape[1], trajs.shape[-1])).astype(theano.config.floatX)\n",
    "\n",
    "traj_mean = trajectories.mean(0)\n",
    "trajc = trajectories[:, :, :, None]\n",
    "trajmm = traj_mean[:, :, None]\n",
    "N = (trajc.shape[0]-1.0)\n",
    "traj_cov = (trajc*trajc.swapaxes(2,3)).sum(0)/N\n",
    "traj_cov -= (trajmm*trajmm.swapaxes(1,2))\n",
    "\n",
    "trajs = theano.shared(trajectories, name='trajs')\n",
    "target_mean = theano.shared(traj_mean, name='target_mean')\n",
    "target_cov = theano.shared(traj_cov, name='target_cov')\n",
    "\n",
    "# define cost as sum of task cost and deviation form expert demonstration\n",
    "def task_plus_il_cost(t, mx, Sx, weights=[1, 1e-4], loss_type=utils.ImitationLossType.KLQP):\n",
    "    '''\n",
    "        The IL term will penalize rollout predictive distributions that \n",
    "        are too different from the target distribution\n",
    "    '''\n",
    "    mxa, Sxa = convert_angle_dimensions(mx, Sx, angle_dims)\n",
    "    mt, St = target_mean[t], target_cov[t]\n",
    "\n",
    "    if loss_type == utils.ImitationLossType.KLQP:\n",
    "        imitation_loss = gaussian_kl_loss(mxa, Sxa, mt, St)\n",
    "    elif loss_type == utils.ImitationLossType.KLPQ:\n",
    "        imitation_loss = gaussian_kl_loss(mt, St, mxa, Sxa)\n",
    "    elif loss_type == utils.ImitationLossType.KLSYM:\n",
    "        imitation_loss = 0.5*(gaussian_kl_loss(mt, St, mxa, Sxa) + gaussian_kl_loss(mxa, Sxa, mt, St))\n",
    "    return weights[0]*task_cost(mx, Sx)[0] + weights[1]*imitation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_shared = [trajs, target_mean, target_cov]\n",
    "rollout_fn = None\n",
    "target_exp = None\n",
    "fig = None\n",
    "axarr = None\n",
    "\n",
    "\n",
    "def learning_iteration_cb(exp, dyn, pol, polopt, params, rollout_fn_in):\n",
    "    global rollout_fn\n",
    "    global target_exp\n",
    "    i = exp.curr_episode\n",
    "    # setup output directory\n",
    "    exp.save(None, 'experience_%d' % (i))\n",
    "    pol.save(None, 'policy_%d' % (i))\n",
    "    dyn.save(None, 'dynamics_%d' % (i))\n",
    "    with open(os.path.join(utils.get_output_dir(), 'config.dill'), 'wb') as f:\n",
    "        dill.dump(params, f)\n",
    "    rollout_fn = rollout_fn_in\n",
    "    target_exp = exp\n",
    "\n",
    "counter = 0\n",
    "def minimize_cb(*args, **kwargs):\n",
    "    global fig\n",
    "    global axarr\n",
    "    global counter\n",
    "    if counter % 500 == 0:\n",
    "        p0 = params['state0_dist']\n",
    "        m0, S0 = p0.mean, p0.cov\n",
    "        fig, axarr = plot_rollout(rollout_fn, source_exp, m0, S0, H, 1.0,\n",
    "                                  fig=fig, axarr=axarr, n_exp=n_demo, name='Rollout during optimization')\n",
    "        plt.waitforbuttonpress(0.01)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['target'] = copy.deepcopy(params['plant'])\n",
    "params['target']['pole_mass'] *= 2\n",
    "params['target']['name'] = 'target_2x_mass'\n",
    "target_env = cartpole.Cartpole(**params['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['target'] = copy.deepcopy(params['plant'])\n",
    "params['target']['pole_length'] *= 2\n",
    "params['target']['name'] = 'target_2x_length'\n",
    "target_env = cartpole.Cartpole(**params['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment 1 learn from scratch\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '001_no_transfer')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=False,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment 2 learn starting from source policy and dynamics\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '002_task_cost_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=False,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 3 learn starting from scratch, using klqp imitation loss\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '003_il_klqp_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 4 learn starting from scratch, using klpq imitation loss\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '004_il_klpq_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 5 learn starting from source params, using klqp imitation loss\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '005_il_klqp_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 6 learn starting from source, using klpq imitation loss\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '004_il_klpq_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 7 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '007_taskplusil_klqp_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 8 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '008_taskplusil_klpq_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 9 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '009_taskplusil_klqp_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 8 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(sim2real_output_dir, target_env.name + '010_taskplusil_klpq_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

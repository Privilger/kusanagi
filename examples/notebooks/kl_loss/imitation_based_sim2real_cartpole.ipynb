{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib qt\n",
    "import copy\n",
    "import dill\n",
    "import os\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from kusanagi import utils\n",
    "from kusanagi.base import apply_controller, ExperienceDataset\n",
    "from kusanagi.ghost import control, regression\n",
    "from kusanagi.shell import cartpole, arduino\n",
    "from kusanagi.shell.cost import gaussian_kl_loss, convert_angle_dimensions\n",
    "from kusanagi.shell.experiment_utils import run_pilco_experiment, setup_mc_pilco_experiment, plot_rollout\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# np.random.seed(1337)\n",
    "np.set_printoptions(linewidth=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init params\n",
    "params = cartpole.default_params()\n",
    "params['optimizer']['min_method'] = 'adam'\n",
    "params['optimizer']['max_evals'] = 1000\n",
    "params['learning_rate'] = 1e-4\n",
    "params['crn_dropout'] = True\n",
    "params['min_steps'] = 30\n",
    "n_samples = 100                     # number of MC samples for bayesian nn\n",
    "n_demo = 10                          # number of example trajectories\n",
    "pol_adjustment = False\n",
    "\n",
    "H = params['min_steps']\n",
    "gamma = params['discount']\n",
    "angle_dims = params['angle_dims']\n",
    "\n",
    "# initial state distribution\n",
    "p0 = params['state0_dist']\n",
    "D = p0.mean.size\n",
    "\n",
    "dyn_path = '/home/juancamilog/.kusanagi/output/cartpole_kl_loss/dynamics_21'\n",
    "pol_path = '/home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21'\n",
    "exp_path = None #'/home/juancamilog/.kusanagi/output/cartpole_kl_loss/experience_29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_dyn(params, dyn_path=None, copy_params=True):\n",
    "\n",
    "    dyn_spec = dict(\n",
    "        hidden_dims=[200]*2,\n",
    "        p=True, p_input=True,\n",
    "        nonlinearities=regression.nonlinearities.rectify,\n",
    "        W_init=lasagne.init.GlorotNormal(gain='relu'),\n",
    "        dropout_class=regression.layers.DenseLogNormalDropoutLayer,\n",
    "        build_fn=regression.dropout_mlp)\n",
    "    \n",
    "    if dyn_path is not None:\n",
    "        # load dynamics model\n",
    "        source_dyn = regression.BNN(\n",
    "            filename=dyn_path, name='source_dyn', **params['dynamics_model'])\n",
    "    else:\n",
    "        # init dynamics model\n",
    "        source_dyn = regression.BNN(network_spec=dyn_spec, name='source_dyn', **params['dynamics_model'])\n",
    "        \n",
    "    if copy_params and dyn_path is not None:\n",
    "        target_dyn = regression.BNN(\n",
    "            filename=dyn_path, name='target_dyn', **params['dynamics_model'])\n",
    "    else:\n",
    "        target_dyn = regression.BNN(network_spec=dyn_spec, name='target_dyn', **params['dynamics_model'])\n",
    "\n",
    "    return source_dyn, target_dyn\n",
    "\n",
    "def init_pol(params,  pol_path=None, adjustment=False, copy_params=True):\n",
    "    pol_spec = dict(\n",
    "        hidden_dims=[200]*2,\n",
    "        p=0.1, p_input=0.0,\n",
    "        nonlinearities=regression.nonlinearities.rectify,\n",
    "        W_init=lasagne.init.GlorotNormal(gain='relu'),\n",
    "        dropout_class=regression.layers.DenseDropoutLayer,\n",
    "        build_fn=regression.dropout_mlp)\n",
    "\n",
    "    if pol_path is not None:\n",
    "        # load policy\n",
    "        source_pol = control.NNPolicy(params['dynamics_model']['odims'], filename=pol_path, **params['policy'])\n",
    "    else:\n",
    "        # init policy\n",
    "        source_pol = control.NNPolicy(\n",
    "            params['dynamics_model']['odims'], network_spec=pol_spec, heteroscedastic=False, **params['policy'])\n",
    "    if pol_adjustment:\n",
    "        # init adjustment model\n",
    "        target_pol = control.AdjustedPolicy(\n",
    "            source_pol, maxU=source_pol.maxU, angle_dims=source_pol.angle_dims,\n",
    "            adjustment_model_class=regression.BNN)\n",
    "        target_pol.adjustment_model.trained = True\n",
    "    else:\n",
    "        if copy_params and pol_path is not None:\n",
    "            target_pol = control.NNPolicy(\n",
    "                params['dynamics_model']['odims'], filename=pol_path, **params['policy'])\n",
    "        else:\n",
    "            target_pol = control.NNPolicy(\n",
    "                params['dynamics_model']['odims'], network_spec=pol_spec, heteroscedastic=False, **params['policy'])\n",
    "            \n",
    "    return source_pol, target_pol\n",
    "\n",
    "# init task cost\n",
    "task_cost = partial(cartpole.cartpole_loss, **params['cost'])\n",
    "\n",
    "# init source environment\n",
    "params['source'] = params['plant']\n",
    "params['source']['name'] = 'Cartpole_src'\n",
    "source_env = cartpole.Cartpole(**params['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 13:48:06.942303] NNPolicy > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21.zip\n",
      "[2018-05-12 13:48:06.957485] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': NNPolicy_fc0>W, 'b': NNPolicy_fc0>b, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_fc1>b, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': NNPolicy_fc1>noise_samples, 'p': 0.1, 'num_units': 200, 'W': NNPolicy_fc1>W})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_output>b, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': NNPolicy_output>noise_samples, 'p': 0.1, 'num_units': 1, 'W': NNPolicy_output>W})\n",
      "[2018-05-12 13:48:06.962923] NNPolicy > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21.zip\n",
      "[2018-05-12 13:48:06.975962] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': NNPolicy_fc0>W, 'b': NNPolicy_fc0>b, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_fc1>b, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': NNPolicy_fc1>noise_samples, 'p': 0.1, 'num_units': 200, 'W': NNPolicy_fc1>W})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_output>b, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': NNPolicy_output>noise_samples, 'p': 0.1, 'num_units': 1, 'W': NNPolicy_output>W})\n",
      "[2018-05-12 13:48:06.980098] Experience > Initialising new experience dataset\n",
      "[2018-05-12 13:48:06.982075] NNPolicy > Initialising expression graph for prediction\n",
      "[2018-05-12 13:48:07.585039] NNPolicy > Compiling mean and variance of prediction\n",
      "[2018-05-12 13:48:07.835074] NNPolicy > Done compiling\n",
      "[2018-05-12 13:48:07.845051] apply_controller > Starting run\n",
      "[2018-05-12 13:48:07.846389] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:08.330166] apply_controller > Done. Stopping robot. Value of run [11.359062]\n",
      "[2018-05-12 13:48:08.331594] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:08.333144] apply_controller > Starting run\n",
      "[2018-05-12 13:48:08.334810] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:08.668191] apply_controller > Done. Stopping robot. Value of run [8.740455]\n",
      "[2018-05-12 13:48:08.669838] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:08.671479] apply_controller > Starting run\n",
      "[2018-05-12 13:48:08.672747] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:09.000343] apply_controller > Done. Stopping robot. Value of run [7.882697]\n",
      "[2018-05-12 13:48:09.001621] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:09.003271] apply_controller > Starting run\n",
      "[2018-05-12 13:48:09.004780] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:09.325555] apply_controller > Done. Stopping robot. Value of run [8.269482]\n",
      "[2018-05-12 13:48:09.326770] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:09.328387] apply_controller > Starting run\n",
      "[2018-05-12 13:48:09.329608] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:09.620582] apply_controller > Done. Stopping robot. Value of run [7.536327]\n",
      "[2018-05-12 13:48:09.622102] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:09.623665] apply_controller > Starting run\n",
      "[2018-05-12 13:48:09.625279] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:09.982823] apply_controller > Done. Stopping robot. Value of run [9.214030]\n",
      "[2018-05-12 13:48:09.984232] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:09.985781] apply_controller > Starting run\n",
      "[2018-05-12 13:48:09.987275] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:10.328805] apply_controller > Done. Stopping robot. Value of run [8.353354]\n",
      "[2018-05-12 13:48:10.330108] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:10.333693] apply_controller > Starting run\n",
      "[2018-05-12 13:48:10.335106] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:10.637024] apply_controller > Done. Stopping robot. Value of run [7.814285]\n",
      "[2018-05-12 13:48:10.638595] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:10.640114] apply_controller > Starting run\n",
      "[2018-05-12 13:48:10.641455] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:11.000073] apply_controller > Done. Stopping robot. Value of run [9.849676]\n",
      "[2018-05-12 13:48:11.001362] Cartpole_src > Stopping robot\n",
      "[2018-05-12 13:48:11.002855] apply_controller > Starting run\n",
      "[2018-05-12 13:48:11.004068] apply_controller > Running for 3.100000 seconds\n",
      "[2018-05-12 13:48:11.336513] apply_controller > Done. Stopping robot. Value of run [8.640543]\n",
      "[2018-05-12 13:48:11.337371] Cartpole_src > Stopping robot\n"
     ]
    }
   ],
   "source": [
    "# collect example trajectory data on sim environment\n",
    "source_pol = init_pol(params, pol_path)[0]\n",
    "if exp_path is not None:\n",
    "    source_exp = ExperienceDataset(filename=exp_path)\n",
    "else:\n",
    "    source_exp = ExperienceDataset()\n",
    "\n",
    "# init expert trajectory variables\n",
    "n_episodes = source_exp.n_episodes()\n",
    "if n_demo > n_episodes:\n",
    "    # function to execute before applying policy\n",
    "    def gTrig(state):\n",
    "        return utils.gTrig_np(state, angle_dims).flatten()\n",
    "\n",
    "    # function to execute after applying policy\n",
    "    def step_cb(state, action, cost, info, env=None):\n",
    "        env.render()\n",
    "\n",
    "    # apply controller\n",
    "    callback = partial(step_cb, env=source_env)\n",
    "\n",
    "    for i in range(n_demo-n_episodes):\n",
    "        ret = apply_controller(source_env, source_pol, H+1, gTrig, callback)\n",
    "        source_exp.append_episode(*ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source trajectory\n",
    "trajs = np.array(source_exp.states)\n",
    "tr_shape = trajs.shape\n",
    "\n",
    "trajs = utils.gTrig_np(trajs.reshape((tr_shape[0]*tr_shape[1], tr_shape[2])), angle_dims)\n",
    "trajectories = trajs.reshape((tr_shape[0], tr_shape[1], trajs.shape[-1])).astype(theano.config.floatX)\n",
    "\n",
    "traj_mean = trajectories.mean(0)\n",
    "trajc = trajectories[:, :, :, None]\n",
    "trajmm = traj_mean[:, :, None]\n",
    "N = (trajc.shape[0]-1.0)\n",
    "traj_cov = (trajc*trajc.swapaxes(2,3)).sum(0)/N\n",
    "traj_cov -= (trajmm*trajmm.swapaxes(1,2))\n",
    "\n",
    "trajs = theano.shared(trajectories, name='trajs')\n",
    "target_mean = theano.shared(traj_mean, name='target_mean')\n",
    "target_cov = theano.shared(traj_cov, name='target_cov')\n",
    "\n",
    "# define cost as sum of task cost and deviation form expert demonstration\n",
    "def task_plus_il_cost(t, mx, Sx, weights=[1, 1e-4], loss_type=utils.ImitationLossType.KLQP):\n",
    "    '''\n",
    "        The IL term will penalize rollout predictive distributions that \n",
    "        are too different from the target distribution\n",
    "    '''\n",
    "    mxa, Sxa = convert_angle_dimensions(mx, Sx, angle_dims)\n",
    "    mt, St = target_mean[t], target_cov[t]\n",
    "\n",
    "    if loss_type == utils.ImitationLossType.KLQP:\n",
    "        imitation_loss = gaussian_kl_loss(mxa, Sxa, mt, St)\n",
    "    elif loss_type == utils.ImitationLossType.KLPQ:\n",
    "        imitation_loss = gaussian_kl_loss(mt, St, mxa, Sxa)\n",
    "    elif loss_type == utils.ImitationLossType.KLSYM:\n",
    "        imitation_loss = 0.5*(gaussian_kl_loss(mt, St, mxa, Sxa) + gaussian_kl_loss(mxa, Sxa, mt, St))\n",
    "    return weights[0]*task_cost(mx, Sx)[0] + weights[1]*imitation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_shared = [trajs, target_mean, target_cov]\n",
    "rollout_fn = None\n",
    "target_exp = None\n",
    "fig = None\n",
    "axarr = None\n",
    "\n",
    "\n",
    "def learning_iteration_cb(exp, dyn, pol, polopt, params, rollout_fn_in):\n",
    "    global rollout_fn\n",
    "    global target_exp\n",
    "    i = exp.curr_episode\n",
    "    # setup output directory\n",
    "    exp.save(None, 'experience_%d' % (i))\n",
    "    pol.save(None, 'policy_%d' % (i))\n",
    "    dyn.save(None, 'dynamics_%d' % (i))\n",
    "    with open(os.path.join(utils.get_output_dir(), 'config.dill'), 'wb') as f:\n",
    "        dill.dump(params, f)\n",
    "    rollout_fn = rollout_fn_in\n",
    "    target_exp = exp\n",
    "\n",
    "counter = 0\n",
    "def minimize_cb(*args, **kwargs):\n",
    "    global fig\n",
    "    global axarr\n",
    "    global counter\n",
    "    if counter % 500 == 0:\n",
    "        p0 = params['state0_dist']\n",
    "        m0, S0 = p0.mean, p0.cov\n",
    "        fig, axarr = plot_rollout(rollout_fn, source_exp, m0, S0, H, 1.0,\n",
    "                                  fig=fig, axarr=axarr, n_exp=n_demo, name='Rollout during optimization')\n",
    "        plt.waitforbuttonpress(0.01)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['target'] = copy.deepcopy(params['plant'])\n",
    "params['target']['pole_mass'] *= 2\n",
    "params['target']['name'] = 'target_2x_mass'\n",
    "target_env = cartpole.Cartpole(**params['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['target'] = copy.deepcopy(params['plant'])\n",
    "params['target']['pole_length'] *= 2\n",
    "params['target']['name'] = 'target_2x_length'\n",
    "target_env = cartpole.Cartpole(**params['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 13:48:27.820705] source_dyn > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/dynamics_21.zip\n",
      "[2018-05-12 13:48:27.852473] source_dyn > Building network\n",
      "('InputLayer', {'shape': (None, 6), 'name': 'BNN_input'})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc0>b, 'name': 'BNN_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc0>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc0>W, 'logit_posterior_mean': BNN_fc0>logit_posterior_mean, 'logit_posterior_std': BNN_fc0>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc1>b, 'name': 'BNN_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc1>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc1>W, 'logit_posterior_mean': BNN_fc1>logit_posterior_mean, 'logit_posterior_std': BNN_fc1>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_output>b, 'name': 'BNN_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': BNN_output>noise_samples, 'p': True, 'num_units': 8, 'W': BNN_output>W, 'logit_posterior_mean': BNN_output>logit_posterior_mean, 'logit_posterior_std': BNN_output>logit_posterior_std})\n",
      "[2018-05-12 13:48:27.896322] NNPolicy > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21.zip\n",
      "[2018-05-12 13:48:27.912782] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': NNPolicy_fc0>W, 'b': NNPolicy_fc0>b, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_fc1>b, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': NNPolicy_fc1>noise_samples, 'p': 0.1, 'num_units': 200, 'W': NNPolicy_fc1>W})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_output>b, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': NNPolicy_output>noise_samples, 'p': 0.1, 'num_units': 1, 'W': NNPolicy_output>W})\n",
      "[2018-05-12 13:48:27.919658] Experience > Initialising new experience dataset\n",
      "[2018-05-12 13:48:27.921168] Executing uniformly-random controls\n",
      "[2018-05-12 13:48:27.922543] apply_controller > Starting run\n",
      "[2018-05-12 13:48:27.924029] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:48:28.094940] apply_controller > Done. Stopping robot. Value of run [29.997318]\n",
      "[2018-05-12 13:48:28.096194] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:48:28.097534] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:48:28.100039] train_dynamics > Dataset size:: Inputs: [ (29, 6) ], Targets: [ (29, 4) ] \n",
      "[2018-05-12 13:48:28.101282] target_dyn > Building network\n",
      "('InputLayer', {'shape': (None, 6), 'name': 'target_dyn_input'})\n",
      "('DenseLogNormalDropoutLayer', {'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'target_dyn_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'p': True, 'num_units': 200, 'W': <lasagne.init.GlorotNormal object at 0x7f1248ee74d0>})\n",
      "('DenseLogNormalDropoutLayer', {'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'target_dyn_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'p': True, 'num_units': 200, 'W': <lasagne.init.GlorotNormal object at 0x7f1248ee74d0>})\n",
      "('DenseLogNormalDropoutLayer', {'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'target_dyn_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'p': True, 'num_units': 8, 'W': <lasagne.init.GlorotNormal object at 0x7f1248ee74d0>})\n",
      "[2018-05-12 13:48:28.134018] target_dyn > Initialising loss function\n",
      "[2018-05-12 13:48:28.285016] target_dyn_opt > Building computation graph for gradients\n",
      "[2018-05-12 13:48:28.570787] target_dyn_opt > No gradient clipping\n",
      "[2018-05-12 13:48:28.572205] target_dyn_opt > Computing parameter update rules\n",
      "[2018-05-12 13:48:28.631914] target_dyn_opt > Compiling function for loss\n",
      "[2018-05-12 13:48:29.506450] target_dyn_opt > Compiling parameter updates\n",
      "[2018-05-12 13:48:35.922714] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:48:35.931107] target_dyn_opt > Initial loss [1637.7912079323764]\n",
      "\u001b[2K[2018-05-12 13:48:45.567246] target_dyn_opt > Curr loss: 5.021232E+01 [1966: 4.852411E+01], n_evals: 1999, Avg. time per updt: 0.003285\n",
      "[2018-05-12 13:48:45.573978] target_dyn_opt > Done training. New loss [52.197227] iter: [2000]\n",
      "[2018-05-12 13:48:45.806659] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:48:45.842610] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': <lasagne.init.GlorotNormal object at 0x7f1248ee73d0>, 'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'p': 0.1, 'num_units': 200, 'W': <lasagne.init.GlorotNormal object at 0x7f1248ee73d0>})\n",
      "('DenseDropoutLayer', {'b': <lasagne.init.Uniform object at 0x7f12be1ec590>, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'p': 0.1, 'num_units': 1, 'W': <lasagne.init.GlorotNormal object at 0x7f1248ee73d0>})\n",
      "[2018-05-12 13:48:46.019799] mc_pilco.rollout > Using common random numbers for moment matching\n",
      "[2018-05-12 13:48:46.021054] mc_pilco.rollout > CRNs will be resampled every 500 rollouts\n",
      "[2018-05-12 13:48:46.111820] mc_pilco.rollout > Building computation graph for rollout\n",
      "[2018-05-12 13:48:46.113179] mc_pilco.rollout > Moment-matching [state: True, cost:True], State measurement noise [policy: True, cost: False]\n",
      "[2018-05-12 13:48:46.596847] mc_pilco.rollout > Using common random numbers for moment matching\n",
      "[2018-05-12 13:48:46.598101] mc_pilco.rollout > CRNs will be resampled every 500 rollouts\n",
      "[2018-05-12 13:48:46.683431] mc_pilco.rollout > Building computation graph for rollout\n",
      "[2018-05-12 13:48:46.685453] mc_pilco.rollout > Moment-matching [state: False, cost:True], State measurement noise [policy: True, cost: False]\n",
      "[2018-05-12 13:48:52.378740] SGDOptimizer > Building computation graph for gradients\n",
      "[2018-05-12 13:48:53.264513] SGDOptimizer > Clipping gradients to norm 1.0\n",
      "[2018-05-12 13:48:53.274555] SGDOptimizer > Computing parameter update rules\n",
      "[2018-05-12 13:48:53.306706] SGDOptimizer > Compiling function for loss\n",
      "[2018-05-12 13:48:56.893691] SGDOptimizer > Compiling parameter updates\n",
      "[2018-05-12 13:49:13.479683] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_0.zip\n",
      "[2018-05-12 13:49:13.548910] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_0.zip\n",
      "[2018-05-12 13:49:13.626033] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_0.zip\n",
      "[2018-05-12 13:49:13.728807] experiment_utils > using common random numbers for dyn and pol\n",
      "[2018-05-12 13:49:13.730125] ==== Iteration [1], experience: [30 steps] ====\n",
      "[2018-05-12 13:49:13.734997] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:49:13.982361] SGDOptimizer > Initial loss [0.999980628490448]\n",
      "\u001b[2K[2018-05-12 13:50:41.177480] SGDOptimizer > Curr loss: 9.767945E-01, n_evals: 999, Avg. time per updt: 0.085889\n",
      "[2018-05-12 13:50:41.204932] SGDOptimizer > Done training. New loss [0.992493] iter: [999]\n",
      "[2018-05-12 13:50:41.206744] NNPolicy > Initialising expression graph for prediction\n",
      "[2018-05-12 13:50:41.284374] NNPolicy > Compiling mean and variance of prediction\n",
      "[2018-05-12 13:50:41.439472] NNPolicy > Done compiling\n",
      "[2018-05-12 13:50:41.441080] apply_controller > Starting run\n",
      "[2018-05-12 13:50:41.442719] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:50:41.651288] apply_controller > Done. Stopping robot. Value of run [29.989243]\n",
      "[2018-05-12 13:50:41.652641] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:50:41.653331] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:50:41.654849] train_dynamics > Dataset size:: Inputs: [ (58, 6) ], Targets: [ (58, 4) ] \n",
      "[2018-05-12 13:50:41.655695] target_dyn_opt > Optimizing parameters via mini batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 13:50:41.665506] target_dyn_opt > Initial loss [133.31895743475843]\n",
      "\u001b[2K[2018-05-12 13:50:54.693628] target_dyn_opt > Curr loss: 2.148382E+01 [1941: 2.060586E+01], n_evals: 1999, Avg. time per updt: 0.004992\n",
      "[2018-05-12 13:50:54.703835] target_dyn_opt > Done training. New loss [24.196404] iter: [2000]\n",
      "[2018-05-12 13:50:54.706563] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:50:54.707951] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_1.zip\n",
      "[2018-05-12 13:50:54.827774] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_1.zip\n",
      "[2018-05-12 13:50:54.903440] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_1.zip\n",
      "[2018-05-12 13:50:56.574932] ==== Iteration [2], experience: [60 steps] ====\n",
      "[2018-05-12 13:50:56.579818] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:50:56.708188] SGDOptimizer > Initial loss [0.9907028675079346]\n",
      "\u001b[2K[2018-05-12 13:52:25.226704] SGDOptimizer > Curr loss: 8.982817E-01, n_evals: 999, Avg. time per updt: 0.087169\n",
      "[2018-05-12 13:52:25.252492] SGDOptimizer > Done training. New loss [0.898212] iter: [999]\n",
      "[2018-05-12 13:52:25.254161] apply_controller > Starting run\n",
      "[2018-05-12 13:52:25.255419] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:52:25.441409] apply_controller > Done. Stopping robot. Value of run [28.914024]\n",
      "[2018-05-12 13:52:25.442749] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:52:25.444248] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:52:25.446823] train_dynamics > Dataset size:: Inputs: [ (87, 6) ], Targets: [ (87, 4) ] \n",
      "[2018-05-12 13:52:25.448380] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:52:25.464724] target_dyn_opt > Initial loss [23.75347039578402]\n",
      "\u001b[2K[2018-05-12 13:52:42.213287] target_dyn_opt > Curr loss: 1.116356E+01 [1904: 1.004841E+01], n_evals: 1999, Avg. time per updt: 0.006809\n",
      "[2018-05-12 13:52:42.226231] target_dyn_opt > Done training. New loss [11.584784] iter: [2000]\n",
      "[2018-05-12 13:52:42.231051] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:52:42.232752] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_2.zip\n",
      "[2018-05-12 13:52:42.395644] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_2.zip\n",
      "[2018-05-12 13:52:42.469163] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_2.zip\n",
      "[2018-05-12 13:52:44.080331] ==== Iteration [3], experience: [90 steps] ====\n",
      "[2018-05-12 13:52:44.085086] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:52:44.207642] SGDOptimizer > Initial loss [0.8957379460334778]\n",
      "\u001b[2K[2018-05-12 13:54:11.859803] SGDOptimizer > Curr loss: 8.793316E-01, n_evals: 999, Avg. time per updt: 0.086302\n",
      "[2018-05-12 13:54:11.882385] SGDOptimizer > Done training. New loss [0.878311] iter: [999]\n",
      "[2018-05-12 13:54:11.884483] apply_controller > Starting run\n",
      "[2018-05-12 13:54:11.885981] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:54:12.059418] apply_controller > Done. Stopping robot. Value of run [27.352001]\n",
      "[2018-05-12 13:54:12.060658] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:54:12.061997] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:54:12.064615] train_dynamics > Dataset size:: Inputs: [ (116, 6) ], Targets: [ (116, 4) ] \n",
      "[2018-05-12 13:54:12.066188] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:54:12.079487] target_dyn_opt > Initial loss [17.398291922500558]\n",
      "\u001b[2K[2018-05-12 13:54:30.140169] target_dyn_opt > Curr loss: 4.981624E+00 [1889: 4.413643E+00], n_evals: 1999, Avg. time per updt: 0.007479\n",
      "[2018-05-12 13:54:30.155082] target_dyn_opt > Done training. New loss [5.162702] iter: [2000]\n",
      "[2018-05-12 13:54:30.157772] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:54:30.159673] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_3.zip\n",
      "[2018-05-12 13:54:30.368266] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_3.zip\n",
      "[2018-05-12 13:54:30.443136] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_3.zip\n",
      "[2018-05-12 13:54:32.068799] ==== Iteration [4], experience: [120 steps] ====\n",
      "[2018-05-12 13:54:32.073797] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:54:32.176729] SGDOptimizer > Initial loss [0.9370357394218445]\n",
      "\u001b[2K[2018-05-12 13:55:58.087245] SGDOptimizer > Curr loss: 8.226938E-01, n_evals: 999, Avg. time per updt: 0.084575\n",
      "[2018-05-12 13:55:58.141590] SGDOptimizer > Done training. New loss [0.817610] iter: [999]\n",
      "[2018-05-12 13:55:58.143601] apply_controller > Starting run\n",
      "[2018-05-12 13:55:58.144862] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:55:58.342348] apply_controller > Done. Stopping robot. Value of run [24.480389]\n",
      "[2018-05-12 13:55:58.343576] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:55:58.346912] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:55:58.349246] train_dynamics > Dataset size:: Inputs: [ (145, 6) ], Targets: [ (145, 4) ] \n",
      "[2018-05-12 13:55:58.350519] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:55:58.366956] target_dyn_opt > Initial loss [5.4063148607612295]\n",
      "\u001b[2K[2018-05-12 13:56:20.540615] target_dyn_opt > Curr loss: 1.695856E+00 [1964: 1.011947E+00], n_evals: 1999, Avg. time per updt: 0.009420\n",
      "[2018-05-12 13:56:20.556230] target_dyn_opt > Done training. New loss [1.223053] iter: [2000]\n",
      "[2018-05-12 13:56:20.558949] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:56:20.560688] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_4.zip\n",
      "[2018-05-12 13:56:20.823462] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_4.zip\n",
      "[2018-05-12 13:56:20.898690] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_4.zip\n",
      "[2018-05-12 13:56:22.588386] ==== Iteration [5], experience: [150 steps] ====\n",
      "[2018-05-12 13:56:22.593533] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:56:22.703487] SGDOptimizer > Initial loss [0.8963897824287415]\n",
      "\u001b[2K[2018-05-12 13:57:51.330522] SGDOptimizer > Curr loss: 7.989265E-01, n_evals: 999, Avg. time per updt: 0.087295\n",
      "[2018-05-12 13:57:51.355060] SGDOptimizer > Done training. New loss [0.788511] iter: [999]\n",
      "[2018-05-12 13:57:51.356739] apply_controller > Starting run\n",
      "[2018-05-12 13:57:51.358063] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:57:51.599597] apply_controller > Done. Stopping robot. Value of run [27.219303]\n",
      "[2018-05-12 13:57:51.600970] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:57:51.602174] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:57:51.606542] train_dynamics > Dataset size:: Inputs: [ (174, 6) ], Targets: [ (174, 4) ] \n",
      "[2018-05-12 13:57:51.607924] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:57:51.623503] target_dyn_opt > Initial loss [6.941560582211643]\n",
      "\u001b[2K[2018-05-12 13:58:20.225385] target_dyn_opt > Curr loss: -1.757902E-01 [1722: -1.121081E+00], n_evals: 1999, Avg. time per updt: 0.012348\n",
      "[2018-05-12 13:58:20.238436] target_dyn_opt > Done training. New loss [-0.234763] iter: [2000]\n",
      "[2018-05-12 13:58:20.241785] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 13:58:20.243362] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_5.zip\n",
      "[2018-05-12 13:58:20.551967] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_5.zip\n",
      "[2018-05-12 13:58:20.627397] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_5.zip\n",
      "[2018-05-12 13:58:21.809375] ==== Iteration [6], experience: [180 steps] ====\n",
      "[2018-05-12 13:58:21.814765] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 13:58:21.924706] SGDOptimizer > Initial loss [0.8088105916976929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K[2018-05-12 13:59:54.301906] SGDOptimizer > Curr loss: 5.898086E-01, n_evals: 999, Avg. time per updt: 0.091044\n",
      "[2018-05-12 13:59:54.330819] SGDOptimizer > Done training. New loss [0.578723] iter: [999]\n",
      "[2018-05-12 13:59:54.332452] apply_controller > Starting run\n",
      "[2018-05-12 13:59:54.333659] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 13:59:54.572054] apply_controller > Done. Stopping robot. Value of run [28.041107]\n",
      "[2018-05-12 13:59:54.573540] target_2x_mass > Stopping robot\n",
      "[2018-05-12 13:59:54.574839] train_dynamics > Training dynamics model\n",
      "[2018-05-12 13:59:54.577669] train_dynamics > Dataset size:: Inputs: [ (203, 6) ], Targets: [ (203, 4) ] \n",
      "[2018-05-12 13:59:54.579039] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 13:59:54.594188] target_dyn_opt > Initial loss [3.9307521544006914]\n",
      "\u001b[2K[2018-05-12 14:00:12.384877] target_dyn_opt > Curr loss: -1.884772E+00 [1909: -2.553112E+00], n_evals: 1999, Avg. time per updt: 0.007386\n",
      "[2018-05-12 14:00:12.398001] target_dyn_opt > Done training. New loss [-2.133128] iter: [2000]\n",
      "[2018-05-12 14:00:12.400767] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:00:12.402236] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_6.zip\n",
      "[2018-05-12 14:00:12.750133] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_6.zip\n",
      "[2018-05-12 14:00:12.828224] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_6.zip\n",
      "[2018-05-12 14:00:14.408857] ==== Iteration [7], experience: [210 steps] ====\n",
      "[2018-05-12 14:00:14.413807] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:00:14.517071] SGDOptimizer > Initial loss [0.7752958536148071]\n",
      "\u001b[2K[2018-05-12 14:01:45.442027] SGDOptimizer > Curr loss: 5.472580E-01, n_evals: 999, Avg. time per updt: 0.089553\n",
      "[2018-05-12 14:01:45.465326] SGDOptimizer > Done training. New loss [0.537995] iter: [999]\n",
      "[2018-05-12 14:01:45.467507] apply_controller > Starting run\n",
      "[2018-05-12 14:01:45.469142] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:01:45.711703] apply_controller > Done. Stopping robot. Value of run [27.010887]\n",
      "[2018-05-12 14:01:45.713278] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:01:45.714904] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:01:45.717664] train_dynamics > Dataset size:: Inputs: [ (232, 6) ], Targets: [ (232, 4) ] \n",
      "[2018-05-12 14:01:45.719137] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:01:45.732886] target_dyn_opt > Initial loss [6.749183835583671]\n",
      "\u001b[2K[2018-05-12 14:02:16.219778] target_dyn_opt > Curr loss: -3.302142E+00 [1966: -3.791089E+00], n_evals: 1999, Avg. time per updt: 0.012989\n",
      "[2018-05-12 14:02:16.234652] target_dyn_opt > Done training. New loss [-3.506425] iter: [2000]\n",
      "[2018-05-12 14:02:16.237319] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:02:16.238867] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_7.zip\n",
      "[2018-05-12 14:02:16.648720] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_7.zip\n",
      "[2018-05-12 14:02:16.729388] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_7.zip\n",
      "[2018-05-12 14:02:18.367837] ==== Iteration [8], experience: [240 steps] ====\n",
      "[2018-05-12 14:02:18.372707] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:02:18.481281] SGDOptimizer > Initial loss [0.8177099823951721]\n",
      "\u001b[2K[2018-05-12 14:03:55.382517] SGDOptimizer > Curr loss: 5.316697E-01, n_evals: 999, Avg. time per updt: 0.095476\n",
      "[2018-05-12 14:03:55.411121] SGDOptimizer > Done training. New loss [0.530740] iter: [999]\n",
      "[2018-05-12 14:03:55.413102] apply_controller > Starting run\n",
      "[2018-05-12 14:03:55.414725] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:03:55.623775] apply_controller > Done. Stopping robot. Value of run [23.310352]\n",
      "[2018-05-12 14:03:55.625527] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:03:55.626748] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:03:55.630179] train_dynamics > Dataset size:: Inputs: [ (261, 6) ], Targets: [ (261, 4) ] \n",
      "[2018-05-12 14:03:55.631608] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:03:55.644837] target_dyn_opt > Initial loss [-2.760821133995629]\n",
      "\u001b[2K[2018-05-12 14:04:14.392444] target_dyn_opt > Curr loss: -4.392597E+00 [1898: -4.916275E+00], n_evals: 1999, Avg. time per updt: 0.007473\n",
      "[2018-05-12 14:04:14.407889] target_dyn_opt > Done training. New loss [-4.568222] iter: [2000]\n",
      "[2018-05-12 14:04:14.410910] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:04:14.413942] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_8.zip\n",
      "[2018-05-12 14:04:14.866433] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_8.zip\n",
      "[2018-05-12 14:04:14.944189] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_8.zip\n",
      "[2018-05-12 14:04:16.576493] ==== Iteration [9], experience: [270 steps] ====\n",
      "[2018-05-12 14:04:16.581436] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:04:16.688369] SGDOptimizer > Initial loss [0.646367073059082]\n",
      "\u001b[2K[2018-05-12 14:05:44.953040] SGDOptimizer > Curr loss: 4.976396E-01, n_evals: 999, Avg. time per updt: 0.086928\n",
      "[2018-05-12 14:05:44.975277] SGDOptimizer > Done training. New loss [0.491870] iter: [999]\n",
      "[2018-05-12 14:05:44.980014] apply_controller > Starting run\n",
      "[2018-05-12 14:05:44.981270] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:05:45.171876] apply_controller > Done. Stopping robot. Value of run [14.861706]\n",
      "[2018-05-12 14:05:45.173515] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:05:45.174897] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:05:45.179558] train_dynamics > Dataset size:: Inputs: [ (290, 6) ], Targets: [ (290, 4) ] \n",
      "[2018-05-12 14:05:45.180871] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:05:45.194967] target_dyn_opt > Initial loss [-3.961901440776499]\n",
      "\u001b[2K[2018-05-12 14:06:03.075767] target_dyn_opt > Curr loss: -5.771954E+00 [1781: -5.922238E+00], n_evals: 1999, Avg. time per updt: 0.007435\n",
      "[2018-05-12 14:06:03.095523] target_dyn_opt > Done training. New loss [-5.046965] iter: [2000]\n",
      "[2018-05-12 14:06:03.099155] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:06:03.102775] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_9.zip\n",
      "[2018-05-12 14:06:03.585910] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_9.zip\n",
      "[2018-05-12 14:06:03.661839] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_9.zip\n",
      "[2018-05-12 14:06:05.291648] ==== Iteration [10], experience: [300 steps] ====\n",
      "[2018-05-12 14:06:05.296595] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:06:05.402486] SGDOptimizer > Initial loss [0.780526340007782]\n",
      "\u001b[2K[2018-05-12 14:07:34.492312] SGDOptimizer > Curr loss: 4.574728E-01, n_evals: 999, Avg. time per updt: 0.087762\n",
      "[2018-05-12 14:07:34.516615] SGDOptimizer > Done training. New loss [0.453023] iter: [999]\n",
      "[2018-05-12 14:07:34.518357] apply_controller > Starting run\n",
      "[2018-05-12 14:07:34.519833] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:07:34.730126] apply_controller > Done. Stopping robot. Value of run [13.672843]\n",
      "[2018-05-12 14:07:34.731536] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:07:34.732862] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:07:34.736493] train_dynamics > Dataset size:: Inputs: [ (319, 6) ], Targets: [ (319, 4) ] \n",
      "[2018-05-12 14:07:34.737816] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:07:34.752975] target_dyn_opt > Initial loss [-4.962764782352274]\n",
      "\u001b[2K[2018-05-12 14:07:53.706496] target_dyn_opt > Curr loss: -6.166340E+00 [1848: -6.756393E+00], n_evals: 1999, Avg. time per updt: 0.007938\n",
      "[2018-05-12 14:07:53.722286] target_dyn_opt > Done training. New loss [-6.424064] iter: [2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 14:07:53.725578] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:07:53.727253] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_10.zip\n",
      "[2018-05-12 14:07:54.285185] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_10.zip\n",
      "[2018-05-12 14:07:54.371614] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_10.zip\n",
      "[2018-05-12 14:07:56.066236] ==== Iteration [11], experience: [330 steps] ====\n",
      "[2018-05-12 14:07:56.071409] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:07:56.185833] SGDOptimizer > Initial loss [0.5162771344184875]\n",
      "\u001b[2K[2018-05-12 14:09:28.329044] SGDOptimizer > Curr loss: 4.539379E-01, n_evals: 999, Avg. time per updt: 0.090816\n",
      "[2018-05-12 14:09:28.355669] SGDOptimizer > Done training. New loss [0.454654] iter: [999]\n",
      "[2018-05-12 14:09:28.357362] apply_controller > Starting run\n",
      "[2018-05-12 14:09:28.358679] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:09:28.549628] apply_controller > Done. Stopping robot. Value of run [15.131266]\n",
      "[2018-05-12 14:09:28.551114] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:09:28.552708] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:09:28.555890] train_dynamics > Dataset size:: Inputs: [ (348, 6) ], Targets: [ (348, 4) ] \n",
      "[2018-05-12 14:09:28.557557] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:09:28.571676] target_dyn_opt > Initial loss [-5.798722012621489]\n",
      "\u001b[2K[2018-05-12 14:09:46.782329] target_dyn_opt > Curr loss: -6.731826E+00 [1726: -7.355528E+00], n_evals: 1999, Avg. time per updt: 0.007580\n",
      "[2018-05-12 14:09:46.796352] target_dyn_opt > Done training. New loss [-6.622485] iter: [2000]\n",
      "[2018-05-12 14:09:46.798986] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:09:46.800889] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_11.zip\n",
      "[2018-05-12 14:09:47.397017] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_11.zip\n",
      "[2018-05-12 14:09:47.476782] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_11.zip\n",
      "[2018-05-12 14:09:49.108552] ==== Iteration [12], experience: [360 steps] ====\n",
      "[2018-05-12 14:09:49.113760] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:09:49.226305] SGDOptimizer > Initial loss [0.5975807905197144]\n",
      "\u001b[2K[2018-05-12 14:11:24.281920] SGDOptimizer > Curr loss: 4.641091E-01, n_evals: 999, Avg. time per updt: 0.093733\n",
      "[2018-05-12 14:11:24.310750] SGDOptimizer > Done training. New loss [0.507203] iter: [999]\n",
      "[2018-05-12 14:11:24.312443] apply_controller > Starting run\n",
      "[2018-05-12 14:11:24.313734] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:11:24.544629] apply_controller > Done. Stopping robot. Value of run [25.927034]\n",
      "[2018-05-12 14:11:24.546129] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:11:24.547983] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:11:24.551076] train_dynamics > Dataset size:: Inputs: [ (377, 6) ], Targets: [ (377, 4) ] \n",
      "[2018-05-12 14:11:24.553183] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:11:24.570232] target_dyn_opt > Initial loss [-3.7575669084485392]\n",
      "\u001b[2K[2018-05-12 14:11:43.231730] target_dyn_opt > Curr loss: -7.474424E+00 [1747: -8.038961E+00], n_evals: 1999, Avg. time per updt: 0.007783\n",
      "[2018-05-12 14:11:43.244203] target_dyn_opt > Done training. New loss [-7.630468] iter: [2000]\n",
      "[2018-05-12 14:11:43.247029] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:11:43.248543] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_12.zip\n",
      "[2018-05-12 14:11:43.877962] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_12.zip\n",
      "[2018-05-12 14:11:43.956921] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_12.zip\n",
      "[2018-05-12 14:11:46.941400] ==== Iteration [13], experience: [390 steps] ====\n",
      "[2018-05-12 14:11:46.946869] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:11:47.057635] SGDOptimizer > Initial loss [0.6879684925079346]\n",
      "\u001b[2K[2018-05-12 14:13:22.135692] SGDOptimizer > Curr loss: 7.037430E-01, n_evals: 999, Avg. time per updt: 0.093772\n",
      "[2018-05-12 14:13:22.164783] SGDOptimizer > Done training. New loss [0.679142] iter: [999]\n",
      "[2018-05-12 14:13:22.166444] apply_controller > Starting run\n",
      "[2018-05-12 14:13:22.167648] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:13:22.351711] apply_controller > Done. Stopping robot. Value of run [12.626383]\n",
      "[2018-05-12 14:13:22.353343] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:13:22.354589] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:13:22.357885] train_dynamics > Dataset size:: Inputs: [ (406, 6) ], Targets: [ (406, 4) ] \n",
      "[2018-05-12 14:13:22.359150] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:13:22.372642] target_dyn_opt > Initial loss [-6.646738331324508]\n",
      "\u001b[2K[2018-05-12 14:13:40.363467] target_dyn_opt > Curr loss: -7.799805E+00 [1822: -8.500655E+00], n_evals: 1999, Avg. time per updt: 0.007516\n",
      "[2018-05-12 14:13:40.376344] target_dyn_opt > Done training. New loss [-8.054054] iter: [2000]\n",
      "[2018-05-12 14:13:40.379757] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:13:40.381565] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_13.zip\n",
      "[2018-05-12 14:13:41.052359] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_13.zip\n",
      "[2018-05-12 14:13:41.128680] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_13.zip\n",
      "[2018-05-12 14:13:42.763209] ==== Iteration [14], experience: [420 steps] ====\n",
      "[2018-05-12 14:13:42.768215] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:13:42.899382] SGDOptimizer > Initial loss [0.7485660314559937]\n",
      "\u001b[2K[2018-05-12 14:15:09.017199] SGDOptimizer > Curr loss: 4.719383E-01, n_evals: 999, Avg. time per updt: 0.084807\n",
      "[2018-05-12 14:15:09.042201] SGDOptimizer > Done training. New loss [0.461195] iter: [999]\n",
      "[2018-05-12 14:15:09.044477] apply_controller > Starting run\n",
      "[2018-05-12 14:15:09.046356] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:15:09.233598] apply_controller > Done. Stopping robot. Value of run [15.191778]\n",
      "[2018-05-12 14:15:09.234944] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:15:09.235976] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:15:09.239920] train_dynamics > Dataset size:: Inputs: [ (435, 6) ], Targets: [ (435, 4) ] \n",
      "[2018-05-12 14:15:09.241699] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:15:09.255893] target_dyn_opt > Initial loss [-6.8581451678755645]\n",
      "\u001b[2K[2018-05-12 14:15:27.515211] target_dyn_opt > Curr loss: -8.575781E+00 [1686: -8.863383E+00], n_evals: 1999, Avg. time per updt: 0.007606\n",
      "[2018-05-12 14:15:27.530546] target_dyn_opt > Done training. New loss [-8.514905] iter: [2000]\n",
      "[2018-05-12 14:15:27.533248] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:15:27.534726] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_14.zip\n",
      "[2018-05-12 14:15:28.246565] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_14.zip\n",
      "[2018-05-12 14:15:28.323388] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_14.zip\n",
      "[2018-05-12 14:15:29.965998] ==== Iteration [15], experience: [450 steps] ====\n",
      "[2018-05-12 14:15:29.971045] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:15:30.095009] SGDOptimizer > Initial loss [0.810951828956604]\n",
      "\u001b[2K[2018-05-12 14:16:56.549963] SGDOptimizer > Curr loss: 4.281161E-01, n_evals: 999, Avg. time per updt: 0.085144\n",
      "[2018-05-12 14:16:56.574421] SGDOptimizer > Done training. New loss [0.428035] iter: [999]\n",
      "[2018-05-12 14:16:56.576112] apply_controller > Starting run\n",
      "[2018-05-12 14:16:56.577728] apply_controller > Running for 3.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 14:16:56.757993] apply_controller > Done. Stopping robot. Value of run [12.591006]\n",
      "[2018-05-12 14:16:56.759208] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:16:56.760530] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:16:56.764369] train_dynamics > Dataset size:: Inputs: [ (464, 6) ], Targets: [ (464, 4) ] \n",
      "[2018-05-12 14:16:56.765777] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:16:56.779575] target_dyn_opt > Initial loss [-8.021506384207772]\n",
      "\u001b[2K[2018-05-12 14:17:14.599769] target_dyn_opt > Curr loss: -8.853582E+00 [1530: -9.528332E+00], n_evals: 1999, Avg. time per updt: 0.007412\n",
      "[2018-05-12 14:17:14.614084] target_dyn_opt > Done training. New loss [-9.181263] iter: [2000]\n",
      "[2018-05-12 14:17:14.616825] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:17:14.618370] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_15.zip\n",
      "[2018-05-12 14:17:15.386064] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_15.zip\n",
      "[2018-05-12 14:17:15.464473] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_15.zip\n",
      "[2018-05-12 14:17:17.103430] ==== Iteration [16], experience: [480 steps] ====\n",
      "[2018-05-12 14:17:17.108509] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:17:17.220756] SGDOptimizer > Initial loss [0.7772687077522278]\n",
      "\u001b[2K[2018-05-12 14:18:44.901784] SGDOptimizer > Curr loss: 4.265309E-01, n_evals: 999, Avg. time per updt: 0.086377\n",
      "[2018-05-12 14:18:44.926834] SGDOptimizer > Done training. New loss [0.425149] iter: [999]\n",
      "[2018-05-12 14:18:44.928592] apply_controller > Starting run\n",
      "[2018-05-12 14:18:44.930119] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:18:45.111985] apply_controller > Done. Stopping robot. Value of run [12.286640]\n",
      "[2018-05-12 14:18:45.113519] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:18:45.114803] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:18:45.118702] train_dynamics > Dataset size:: Inputs: [ (493, 6) ], Targets: [ (493, 4) ] \n",
      "[2018-05-12 14:18:45.120079] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:18:45.133690] target_dyn_opt > Initial loss [-8.458228193268814]\n",
      "\u001b[2K[2018-05-12 14:19:03.110034] target_dyn_opt > Curr loss: -9.149007E+00 [1802: -9.816547E+00], n_evals: 1999, Avg. time per updt: 0.007495\n",
      "[2018-05-12 14:19:03.125483] target_dyn_opt > Done training. New loss [-9.461519] iter: [2000]\n",
      "[2018-05-12 14:19:03.128128] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:19:03.129800] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_16.zip\n",
      "[2018-05-12 14:19:03.930834] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_16.zip\n",
      "[2018-05-12 14:19:04.008598] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_16.zip\n",
      "[2018-05-12 14:19:05.682732] ==== Iteration [17], experience: [510 steps] ====\n",
      "[2018-05-12 14:19:05.688312] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:19:05.810650] SGDOptimizer > Initial loss [0.6139024496078491]\n",
      "\u001b[2K[2018-05-12 14:20:33.444596] SGDOptimizer > Curr loss: 4.060147E-01, n_evals: 999, Avg. time per updt: 0.086311\n",
      "[2018-05-12 14:20:33.472257] SGDOptimizer > Done training. New loss [0.405450] iter: [999]\n",
      "[2018-05-12 14:20:33.474141] apply_controller > Starting run\n",
      "[2018-05-12 14:20:33.475399] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:20:33.661317] apply_controller > Done. Stopping robot. Value of run [11.909310]\n",
      "[2018-05-12 14:20:33.662806] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:20:33.665088] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:20:33.668568] train_dynamics > Dataset size:: Inputs: [ (522, 6) ], Targets: [ (522, 4) ] \n",
      "[2018-05-12 14:20:33.670168] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:20:33.686508] target_dyn_opt > Initial loss [-9.003739641772377]\n",
      "\u001b[2K[2018-05-12 14:20:51.781620] target_dyn_opt > Curr loss: -9.170367E+00 [1956: -1.018276E+01], n_evals: 1999, Avg. time per updt: 0.007534\n",
      "[2018-05-12 14:20:51.796383] target_dyn_opt > Done training. New loss [-9.855054] iter: [2000]\n",
      "[2018-05-12 14:20:51.799131] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:20:51.800570] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_17.zip\n",
      "[2018-05-12 14:20:52.650746] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_17.zip\n",
      "[2018-05-12 14:20:52.730201] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_17.zip\n",
      "[2018-05-12 14:20:54.370142] ==== Iteration [18], experience: [540 steps] ====\n",
      "[2018-05-12 14:20:54.375509] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:20:54.487702] SGDOptimizer > Initial loss [0.44044002890586853]\n",
      "\u001b[2K[2018-05-12 14:22:23.085774] SGDOptimizer > Curr loss: 3.973947E-01, n_evals: 999, Avg. time per updt: 0.087276\n",
      "[2018-05-12 14:22:23.111977] SGDOptimizer > Done training. New loss [0.398215] iter: [999]\n",
      "[2018-05-12 14:22:23.113677] apply_controller > Starting run\n",
      "[2018-05-12 14:22:23.114983] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:22:23.294649] apply_controller > Done. Stopping robot. Value of run [22.625633]\n",
      "[2018-05-12 14:22:23.296005] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:22:23.297316] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:22:23.301363] train_dynamics > Dataset size:: Inputs: [ (551, 6) ], Targets: [ (551, 4) ] \n",
      "[2018-05-12 14:22:23.302774] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:22:23.317019] target_dyn_opt > Initial loss [-8.629200520565202]\n",
      "\u001b[2K[2018-05-12 14:22:41.674976] target_dyn_opt > Curr loss: -9.882008E+00 [393: -1.039965E+01], n_evals: 1999, Avg. time per updt: 0.007678\n",
      "[2018-05-12 14:22:41.690417] target_dyn_opt > Done training. New loss [-9.874857] iter: [2000]\n",
      "[2018-05-12 14:22:41.693165] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:22:41.694584] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_18.zip\n",
      "[2018-05-12 14:22:42.601117] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_18.zip\n",
      "[2018-05-12 14:22:42.679784] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_18.zip\n",
      "[2018-05-12 14:22:44.331778] ==== Iteration [19], experience: [570 steps] ====\n",
      "[2018-05-12 14:22:44.336790] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:22:44.448622] SGDOptimizer > Initial loss [0.4601793587207794]\n",
      "\u001b[2K[2018-05-12 14:24:14.219878] SGDOptimizer > Curr loss: 4.146219E-01, n_evals: 999, Avg. time per updt: 0.088436\n",
      "[2018-05-12 14:24:14.245153] SGDOptimizer > Done training. New loss [0.411866] iter: [999]\n",
      "[2018-05-12 14:24:14.246760] apply_controller > Starting run\n",
      "[2018-05-12 14:24:14.248036] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:24:14.427474] apply_controller > Done. Stopping robot. Value of run [12.866420]\n",
      "[2018-05-12 14:24:14.428828] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:24:14.430134] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:24:14.433871] train_dynamics > Dataset size:: Inputs: [ (580, 6) ], Targets: [ (580, 4) ] \n",
      "[2018-05-12 14:24:14.435327] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:24:14.448525] target_dyn_opt > Initial loss [-9.342065557940252]\n",
      "\u001b[2K[2018-05-12 14:24:32.644096] target_dyn_opt > Curr loss: -1.027274E+01 [1180: -1.070734E+01], n_evals: 1999, Avg. time per updt: 0.007591\n",
      "[2018-05-12 14:24:32.658447] target_dyn_opt > Done training. New loss [-10.157380] iter: [2000]\n",
      "[2018-05-12 14:24:32.661283] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:24:32.662860] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_19.zip\n",
      "[2018-05-12 14:24:33.613115] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_19.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 14:24:33.695322] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_19.zip\n",
      "[2018-05-12 14:24:35.357790] ==== Iteration [20], experience: [600 steps] ====\n",
      "[2018-05-12 14:24:35.362856] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:24:35.481892] SGDOptimizer > Initial loss [0.42585092782974243]\n",
      "\u001b[2K[2018-05-12 14:26:04.747751] SGDOptimizer > Curr loss: 4.073306E-01, n_evals: 999, Avg. time per updt: 0.087962\n",
      "[2018-05-12 14:26:04.774754] SGDOptimizer > Done training. New loss [0.409185] iter: [999]\n",
      "[2018-05-12 14:26:04.776715] apply_controller > Starting run\n",
      "[2018-05-12 14:26:04.777917] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:26:04.962077] apply_controller > Done. Stopping robot. Value of run [25.213491]\n",
      "[2018-05-12 14:26:04.963437] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:26:04.964738] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:26:04.968480] train_dynamics > Dataset size:: Inputs: [ (609, 6) ], Targets: [ (609, 4) ] \n",
      "[2018-05-12 14:26:04.970081] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:26:04.984113] target_dyn_opt > Initial loss [-8.141112874465428]\n",
      "\u001b[2K[2018-05-12 14:26:22.701667] target_dyn_opt > Curr loss: -1.037911E+01 [1696: -1.091468E+01], n_evals: 1999, Avg. time per updt: 0.007381\n",
      "[2018-05-12 14:26:22.716701] target_dyn_opt > Done training. New loss [-10.587672] iter: [2000]\n",
      "[2018-05-12 14:26:22.719484] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:26:22.721491] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_20.zip\n",
      "[2018-05-12 14:26:23.705767] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_20.zip\n",
      "[2018-05-12 14:26:23.789726] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_20.zip\n",
      "[2018-05-12 14:26:25.457938] ==== Iteration [21], experience: [630 steps] ====\n",
      "[2018-05-12 14:26:25.463334] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:26:25.581760] SGDOptimizer > Initial loss [0.6128632426261902]\n",
      "\u001b[2K[2018-05-12 14:27:54.508622] SGDOptimizer > Curr loss: 4.056492E-01, n_evals: 999, Avg. time per updt: 0.087593\n",
      "[2018-05-12 14:27:54.534023] SGDOptimizer > Done training. New loss [0.402896] iter: [999]\n",
      "[2018-05-12 14:27:54.536051] apply_controller > Starting run\n",
      "[2018-05-12 14:27:54.537381] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:27:54.783048] apply_controller > Done. Stopping robot. Value of run [12.472166]\n",
      "[2018-05-12 14:27:54.784276] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:27:54.785794] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:27:54.790041] train_dynamics > Dataset size:: Inputs: [ (638, 6) ], Targets: [ (638, 4) ] \n",
      "[2018-05-12 14:27:54.791469] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:27:54.807167] target_dyn_opt > Initial loss [-9.19519886561322]\n",
      "\u001b[2K[2018-05-12 14:28:12.967803] target_dyn_opt > Curr loss: -1.054781E+01 [646: -1.123052E+01], n_evals: 1999, Avg. time per updt: 0.007581\n",
      "[2018-05-12 14:28:12.981554] target_dyn_opt > Done training. New loss [-10.837178] iter: [2000]\n",
      "[2018-05-12 14:28:12.984299] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:28:12.985989] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_21.zip\n",
      "[2018-05-12 14:28:14.016723] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_21.zip\n",
      "[2018-05-12 14:28:14.096873] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_21.zip\n",
      "[2018-05-12 14:28:15.753700] ==== Iteration [22], experience: [660 steps] ====\n",
      "[2018-05-12 14:28:15.758701] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:28:15.875493] SGDOptimizer > Initial loss [0.4380064010620117]\n",
      "\u001b[2K[2018-05-12 14:29:44.946954] SGDOptimizer > Curr loss: 4.001762E-01, n_evals: 999, Avg. time per updt: 0.087736\n",
      "[2018-05-12 14:29:44.973293] SGDOptimizer > Done training. New loss [0.398335] iter: [999]\n",
      "[2018-05-12 14:29:44.975003] apply_controller > Starting run\n",
      "[2018-05-12 14:29:44.976201] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:29:45.159904] apply_controller > Done. Stopping robot. Value of run [12.198632]\n",
      "[2018-05-12 14:29:45.161273] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:29:45.162773] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:29:45.166640] train_dynamics > Dataset size:: Inputs: [ (667, 6) ], Targets: [ (667, 4) ] \n",
      "[2018-05-12 14:29:45.168325] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:29:45.182521] target_dyn_opt > Initial loss [-9.8567240140338]\n",
      "\u001b[2K[2018-05-12 14:30:03.838971] target_dyn_opt > Curr loss: -1.055072E+01 [1582: -1.150478E+01], n_evals: 1999, Avg. time per updt: 0.007808\n",
      "[2018-05-12 14:30:03.853843] target_dyn_opt > Done training. New loss [-11.021903] iter: [2000]\n",
      "[2018-05-12 14:30:03.856453] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:30:03.858077] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_22.zip\n",
      "[2018-05-12 14:30:04.943171] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_22.zip\n",
      "[2018-05-12 14:30:05.023099] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_22.zip\n",
      "[2018-05-12 14:30:06.704778] ==== Iteration [23], experience: [690 steps] ====\n",
      "[2018-05-12 14:30:06.710131] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:30:06.837672] SGDOptimizer > Initial loss [0.4184860289096832]\n",
      "\u001b[2K[2018-05-12 14:31:36.056506] SGDOptimizer > Curr loss: 4.063801E-01, n_evals: 999, Avg. time per updt: 0.087902\n",
      "[2018-05-12 14:31:36.084836] SGDOptimizer > Done training. New loss [0.402716] iter: [999]\n",
      "[2018-05-12 14:31:36.086514] apply_controller > Starting run\n",
      "[2018-05-12 14:31:36.087791] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:31:36.293510] apply_controller > Done. Stopping robot. Value of run [25.325409]\n",
      "[2018-05-12 14:31:36.294849] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:31:36.296330] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:31:36.300224] train_dynamics > Dataset size:: Inputs: [ (696, 6) ], Targets: [ (696, 4) ] \n",
      "[2018-05-12 14:31:36.301620] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:31:36.318237] target_dyn_opt > Initial loss [4.831201269109359]\n",
      "\u001b[2K[2018-05-12 14:31:54.420676] target_dyn_opt > Curr loss: -1.114062E+01 [1184: -1.154988E+01], n_evals: 1999, Avg. time per updt: 0.007557\n",
      "[2018-05-12 14:31:54.434206] target_dyn_opt > Done training. New loss [-11.028251] iter: [2000]\n",
      "[2018-05-12 14:31:54.436935] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:31:54.438390] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_23.zip\n",
      "[2018-05-12 14:31:55.573476] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_23.zip\n",
      "[2018-05-12 14:31:55.654523] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_23.zip\n",
      "[2018-05-12 14:31:57.303012] ==== Iteration [24], experience: [720 steps] ====\n",
      "[2018-05-12 14:31:57.308360] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:31:57.436211] SGDOptimizer > Initial loss [0.7983968257904053]\n",
      "\u001b[2K[2018-05-12 14:33:26.696860] SGDOptimizer > Curr loss: 4.034607E-01, n_evals: 999, Avg. time per updt: 0.087958\n",
      "[2018-05-12 14:33:26.723065] SGDOptimizer > Done training. New loss [0.401959] iter: [999]\n",
      "[2018-05-12 14:33:26.724751] apply_controller > Starting run\n",
      "[2018-05-12 14:33:26.726274] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:33:26.906215] apply_controller > Done. Stopping robot. Value of run [12.505912]\n",
      "[2018-05-12 14:33:26.907582] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:33:26.908691] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:33:26.913918] train_dynamics > Dataset size:: Inputs: [ (725, 6) ], Targets: [ (725, 4) ] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 14:33:26.914956] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:33:26.930791] target_dyn_opt > Initial loss [-9.375177718359849]\n",
      "\u001b[2K[2018-05-12 14:33:44.861823] target_dyn_opt > Curr loss: -1.104130E+01 [1715: -1.175312E+01], n_evals: 1999, Avg. time per updt: 0.007459\n",
      "[2018-05-12 14:33:44.877446] target_dyn_opt > Done training. New loss [-11.465817] iter: [2000]\n",
      "[2018-05-12 14:33:44.880090] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:33:44.881748] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_24.zip\n",
      "[2018-05-12 14:33:46.048106] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_24.zip\n",
      "[2018-05-12 14:33:46.129061] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_24.zip\n",
      "[2018-05-12 14:33:47.802791] ==== Iteration [25], experience: [750 steps] ====\n",
      "[2018-05-12 14:33:47.807852] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:33:47.939445] SGDOptimizer > Initial loss [0.4439820647239685]\n",
      "\u001b[2K[2018-05-12 14:35:18.797015] SGDOptimizer > Curr loss: 4.382968E-01, n_evals: 999, Avg. time per updt: 0.089554\n",
      "[2018-05-12 14:35:18.823156] SGDOptimizer > Done training. New loss [0.431331] iter: [999]\n",
      "[2018-05-12 14:35:18.824834] apply_controller > Starting run\n",
      "[2018-05-12 14:35:18.826131] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:35:19.012545] apply_controller > Done. Stopping robot. Value of run [12.570988]\n",
      "[2018-05-12 14:35:19.014065] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:35:19.015582] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:35:19.020196] train_dynamics > Dataset size:: Inputs: [ (754, 6) ], Targets: [ (754, 4) ] \n",
      "[2018-05-12 14:35:19.021434] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:35:19.037404] target_dyn_opt > Initial loss [-9.768289970742929]\n",
      "\u001b[2K[2018-05-12 14:35:37.051391] target_dyn_opt > Curr loss: -1.140078E+01 [1869: -1.203013E+01], n_evals: 1999, Avg. time per updt: 0.007512\n",
      "[2018-05-12 14:35:37.064414] target_dyn_opt > Done training. New loss [-11.310016] iter: [2000]\n",
      "[2018-05-12 14:35:37.067038] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:35:37.068479] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_25.zip\n",
      "[2018-05-12 14:35:38.305824] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_25.zip\n",
      "[2018-05-12 14:35:38.390620] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_25.zip\n",
      "[2018-05-12 14:35:40.068503] ==== Iteration [26], experience: [780 steps] ====\n",
      "[2018-05-12 14:35:40.073839] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:35:40.202052] SGDOptimizer > Initial loss [0.48131129145622253]\n",
      "\u001b[2K[2018-05-12 14:37:12.000067] SGDOptimizer > Curr loss: 3.981478E-01, n_evals: 999, Avg. time per updt: 0.090470\n",
      "[2018-05-12 14:37:12.029428] SGDOptimizer > Done training. New loss [0.429942] iter: [999]\n",
      "[2018-05-12 14:37:12.031557] apply_controller > Starting run\n",
      "[2018-05-12 14:37:12.033213] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:37:12.214481] apply_controller > Done. Stopping robot. Value of run [12.395530]\n",
      "[2018-05-12 14:37:12.215837] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:37:12.217357] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:37:12.222413] train_dynamics > Dataset size:: Inputs: [ (783, 6) ], Targets: [ (783, 4) ] \n",
      "[2018-05-12 14:37:12.223790] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:37:12.237263] target_dyn_opt > Initial loss [-9.929619797763545]\n",
      "\u001b[2K[2018-05-12 14:37:30.522917] target_dyn_opt > Curr loss: -1.172846E+01 [1537: -1.225715E+01], n_evals: 1999, Avg. time per updt: 0.007636\n",
      "[2018-05-12 14:37:30.535973] target_dyn_opt > Done training. New loss [-11.915775] iter: [2000]\n",
      "[2018-05-12 14:37:30.538769] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:37:30.540256] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_26.zip\n",
      "[2018-05-12 14:37:31.815370] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_26.zip\n",
      "[2018-05-12 14:37:31.897679] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_26.zip\n",
      "[2018-05-12 14:37:33.580326] ==== Iteration [27], experience: [810 steps] ====\n",
      "[2018-05-12 14:37:33.584897] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:37:33.717695] SGDOptimizer > Initial loss [0.48393353819847107]\n",
      "\u001b[2K[2018-05-12 14:39:03.762048] SGDOptimizer > Curr loss: 3.987440E-01, n_evals: 999, Avg. time per updt: 0.088729\n",
      "[2018-05-12 14:39:03.787507] SGDOptimizer > Done training. New loss [0.400792] iter: [999]\n",
      "[2018-05-12 14:39:03.789079] apply_controller > Starting run\n",
      "[2018-05-12 14:39:03.790308] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:39:03.986623] apply_controller > Done. Stopping robot. Value of run [14.728552]\n",
      "[2018-05-12 14:39:03.988225] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:39:03.989423] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:39:03.993683] train_dynamics > Dataset size:: Inputs: [ (812, 6) ], Targets: [ (812, 4) ] \n",
      "[2018-05-12 14:39:03.995009] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:39:04.008425] target_dyn_opt > Initial loss [-9.631016365009282]\n",
      "\u001b[2K[2018-05-12 14:39:22.127197] target_dyn_opt > Curr loss: -1.179839E+01 [614: -1.232873E+01], n_evals: 1999, Avg. time per updt: 0.007567\n",
      "[2018-05-12 14:39:22.142000] target_dyn_opt > Done training. New loss [-11.701201] iter: [2000]\n",
      "[2018-05-12 14:39:22.144811] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:39:22.146366] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_27.zip\n",
      "[2018-05-12 14:39:23.453798] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_27.zip\n",
      "[2018-05-12 14:39:23.538111] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_27.zip\n",
      "[2018-05-12 14:39:25.225837] ==== Iteration [28], experience: [840 steps] ====\n",
      "[2018-05-12 14:39:25.231478] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:39:25.360497] SGDOptimizer > Initial loss [0.4052334725856781]\n",
      "\u001b[2K[2018-05-12 14:40:54.748153] SGDOptimizer > Curr loss: 4.074892E-01, n_evals: 999, Avg. time per updt: 0.088082\n",
      "[2018-05-12 14:40:54.773913] SGDOptimizer > Done training. New loss [0.405284] iter: [999]\n",
      "[2018-05-12 14:40:54.775741] apply_controller > Starting run\n",
      "[2018-05-12 14:40:54.777112] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:40:54.969674] apply_controller > Done. Stopping robot. Value of run [25.642748]\n",
      "[2018-05-12 14:40:54.970913] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:40:54.972234] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:40:54.978165] train_dynamics > Dataset size:: Inputs: [ (841, 6) ], Targets: [ (841, 4) ] \n",
      "[2018-05-12 14:40:54.979578] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:40:54.994312] target_dyn_opt > Initial loss [-8.160771616786612]\n",
      "\u001b[2K[2018-05-12 14:41:13.275052] target_dyn_opt > Curr loss: -1.172149E+01 [1929: -1.242371E+01], n_evals: 1999, Avg. time per updt: 0.007635\n",
      "[2018-05-12 14:41:13.287940] target_dyn_opt > Done training. New loss [-12.074276] iter: [2000]\n",
      "[2018-05-12 14:41:13.290694] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:41:13.292109] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_28.zip\n",
      "[2018-05-12 14:41:14.647589] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_28.zip\n",
      "[2018-05-12 14:41:14.735148] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_28.zip\n",
      "[2018-05-12 14:41:16.411634] ==== Iteration [29], experience: [870 steps] ====\n",
      "[2018-05-12 14:41:16.417427] SGDOptimizer > Optimizing parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 14:41:16.533173] SGDOptimizer > Initial loss [0.4186825752258301]\n",
      "\u001b[2K[2018-05-12 14:42:46.431938] SGDOptimizer > Curr loss: 3.934833E-01, n_evals: 999, Avg. time per updt: 0.088571\n",
      "[2018-05-12 14:42:46.457588] SGDOptimizer > Done training. New loss [0.398087] iter: [999]\n",
      "[2018-05-12 14:42:46.459452] apply_controller > Starting run\n",
      "[2018-05-12 14:42:46.461329] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:42:46.689373] apply_controller > Done. Stopping robot. Value of run [10.834880]\n",
      "[2018-05-12 14:42:46.690735] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:42:46.691964] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:42:46.698671] train_dynamics > Dataset size:: Inputs: [ (870, 6) ], Targets: [ (870, 4) ] \n",
      "[2018-05-12 14:42:46.700143] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:42:46.714189] target_dyn_opt > Initial loss [-10.753128531643714]\n",
      "\u001b[2K[2018-05-12 14:43:04.900845] target_dyn_opt > Curr loss: -1.197830E+01 [1482: -1.267330E+01], n_evals: 1999, Avg. time per updt: 0.007601\n",
      "[2018-05-12 14:43:04.916262] target_dyn_opt > Done training. New loss [-11.872853] iter: [2000]\n",
      "[2018-05-12 14:43:04.919133] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:43:04.920943] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_29.zip\n",
      "[2018-05-12 14:43:06.340785] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_29.zip\n",
      "[2018-05-12 14:43:06.422234] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_29.zip\n",
      "[2018-05-12 14:43:08.122003] ==== Iteration [30], experience: [900 steps] ====\n",
      "[2018-05-12 14:43:08.127099] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 14:43:08.243144] SGDOptimizer > Initial loss [0.42809930443763733]\n",
      "\u001b[2K[2018-05-12 14:44:37.718933] SGDOptimizer > Curr loss: 3.995668E-01, n_evals: 999, Avg. time per updt: 0.088141\n",
      "[2018-05-12 14:44:37.746113] SGDOptimizer > Done training. New loss [0.398597] iter: [999]\n",
      "[2018-05-12 14:44:37.747994] apply_controller > Starting run\n",
      "[2018-05-12 14:44:37.749394] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 14:44:37.929976] apply_controller > Done. Stopping robot. Value of run [12.381523]\n",
      "[2018-05-12 14:44:37.931498] target_2x_mass > Stopping robot\n",
      "[2018-05-12 14:44:37.932852] train_dynamics > Training dynamics model\n",
      "[2018-05-12 14:44:37.938346] train_dynamics > Dataset size:: Inputs: [ (899, 6) ], Targets: [ (899, 4) ] \n",
      "[2018-05-12 14:44:37.939720] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 14:44:37.955391] target_dyn_opt > Initial loss [-10.364170648331637]\n",
      "\u001b[2K[2018-05-12 14:44:56.068435] target_dyn_opt > Curr loss: -1.237417E+01 [1080: -1.283730E+01], n_evals: 1999, Avg. time per updt: 0.007564\n",
      "[2018-05-12 14:44:56.081549] target_dyn_opt > Done training. New loss [-11.745076] iter: [2000]\n",
      "[2018-05-12 14:44:56.084257] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 14:44:56.086047] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/experience_30.zip\n",
      "[2018-05-12 14:44:57.536768] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/policy_30.zip\n",
      "[2018-05-12 14:44:57.619367] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/dynamics_30.zip\n"
     ]
    }
   ],
   "source": [
    "# experiment 1 learn from scratch\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '001_no_transfer')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=False,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 15:14:27.592466] source_dyn > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/dynamics_21.zip\n",
      "[2018-05-12 15:14:27.619675] source_dyn > Building network\n",
      "('InputLayer', {'shape': (None, 6), 'name': 'BNN_input'})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc0>b, 'name': 'BNN_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc0>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc0>W, 'logit_posterior_mean': BNN_fc0>logit_posterior_mean, 'logit_posterior_std': BNN_fc0>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc1>b, 'name': 'BNN_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc1>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc1>W, 'logit_posterior_mean': BNN_fc1>logit_posterior_mean, 'logit_posterior_std': BNN_fc1>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_output>b, 'name': 'BNN_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': BNN_output>noise_samples, 'p': True, 'num_units': 8, 'W': BNN_output>W, 'logit_posterior_mean': BNN_output>logit_posterior_mean, 'logit_posterior_std': BNN_output>logit_posterior_std})\n",
      "[2018-05-12 15:14:27.649880] target_dyn > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/dynamics_21.zip\n",
      "[2018-05-12 15:14:27.671635] target_dyn > Building network\n",
      "('InputLayer', {'shape': (None, 6), 'name': 'BNN_input'})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc0>b, 'name': 'BNN_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc0>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc0>W, 'logit_posterior_mean': BNN_fc0>logit_posterior_mean, 'logit_posterior_std': BNN_fc0>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_fc1>b, 'name': 'BNN_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': BNN_fc1>noise_samples, 'p': True, 'num_units': 200, 'W': BNN_fc1>W, 'logit_posterior_mean': BNN_fc1>logit_posterior_mean, 'logit_posterior_std': BNN_fc1>logit_posterior_std})\n",
      "('DenseLogNormalDropoutLayer', {'b': BNN_output>b, 'name': 'BNN_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': BNN_output>noise_samples, 'p': True, 'num_units': 8, 'W': BNN_output>W, 'logit_posterior_mean': BNN_output>logit_posterior_mean, 'logit_posterior_std': BNN_output>logit_posterior_std})\n",
      "[2018-05-12 15:14:27.700093] NNPolicy > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21.zip\n",
      "[2018-05-12 15:14:27.722462] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': NNPolicy_fc0>W, 'b': NNPolicy_fc0>b, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_fc1>b, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': NNPolicy_fc1>noise_samples, 'p': 0.1, 'num_units': 200, 'W': NNPolicy_fc1>W})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_output>b, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': NNPolicy_output>noise_samples, 'p': 0.1, 'num_units': 1, 'W': NNPolicy_output>W})\n",
      "[2018-05-12 15:14:27.728325] NNPolicy > Loading state from /home/juancamilog/.kusanagi/output/cartpole_kl_loss/policy_21.zip\n",
      "[2018-05-12 15:14:27.746239] NNPolicy > Building network\n",
      "('InputLayer', {'shape': (None, 5), 'name': 'NNPolicy_input'})\n",
      "('DenseLayer', {'W': NNPolicy_fc0>W, 'b': NNPolicy_fc0>b, 'name': 'NNPolicy_fc0', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'num_units': 200})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_fc1>b, 'name': 'NNPolicy_fc1', 'nonlinearity': <function rectify at 0x7f12bebeeb90>, 'noise_samples': NNPolicy_fc1>noise_samples, 'p': 0.1, 'num_units': 200, 'W': NNPolicy_fc1>W})\n",
      "('DenseDropoutLayer', {'b': NNPolicy_output>b, 'name': 'NNPolicy_output', 'nonlinearity': <function linear at 0x7f12bebf6050>, 'noise_samples': NNPolicy_output>noise_samples, 'p': 0.1, 'num_units': 1, 'W': NNPolicy_output>W})\n",
      "[2018-05-12 15:14:27.751279] Experience > Initialising new experience dataset\n",
      "[2018-05-12 15:14:27.752587] Executing initial policy\n",
      "[2018-05-12 15:14:27.754313] NNPolicy > Initialising expression graph for prediction\n",
      "[2018-05-12 15:14:27.838815] NNPolicy > Compiling mean and variance of prediction\n",
      "[2018-05-12 15:14:27.996814] NNPolicy > Done compiling\n",
      "[2018-05-12 15:14:27.998421] apply_controller > Starting run\n",
      "[2018-05-12 15:14:28.001647] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:14:28.192428] apply_controller > Done. Stopping robot. Value of run [29.144878]\n",
      "[2018-05-12 15:14:28.193650] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:14:28.194945] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:14:28.197506] train_dynamics > Dataset size:: Inputs: [ (29, 6) ], Targets: [ (29, 4) ] \n",
      "[2018-05-12 15:14:28.199108] target_dyn > Initialising loss function\n",
      "[2018-05-12 15:14:28.351197] target_dyn_opt > Building computation graph for gradients\n",
      "[2018-05-12 15:14:28.654688] target_dyn_opt > No gradient clipping\n",
      "[2018-05-12 15:14:28.656074] target_dyn_opt > Computing parameter update rules\n",
      "[2018-05-12 15:14:28.724547] target_dyn_opt > Compiling function for loss\n",
      "[2018-05-12 15:14:29.631397] target_dyn_opt > Compiling parameter updates\n",
      "[2018-05-12 15:14:34.652870] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:14:34.661681] target_dyn_opt > Initial loss [2373.285944301507]\n",
      "\u001b[2K[2018-05-12 15:14:44.530849] target_dyn_opt > Curr loss: 4.309333E+01 [1886: 4.229093E+01], n_evals: 1999, Avg. time per updt: 0.003367\n",
      "[2018-05-12 15:14:44.538282] target_dyn_opt > Done training. New loss [43.288689] iter: [2000]\n",
      "[2018-05-12 15:14:44.777245] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:14:44.978193] mc_pilco.rollout > Using common random numbers for moment matching\n",
      "[2018-05-12 15:14:44.979865] mc_pilco.rollout > CRNs will be resampled every 500 rollouts\n",
      "[2018-05-12 15:14:45.069126] mc_pilco.rollout > Building computation graph for rollout\n",
      "[2018-05-12 15:14:45.070480] mc_pilco.rollout > Moment-matching [state: True, cost:True], State measurement noise [policy: True, cost: False]\n",
      "[2018-05-12 15:14:45.580652] mc_pilco.rollout > Using common random numbers for moment matching\n",
      "[2018-05-12 15:14:45.581951] mc_pilco.rollout > CRNs will be resampled every 500 rollouts\n",
      "[2018-05-12 15:14:45.681899] mc_pilco.rollout > Building computation graph for rollout\n",
      "[2018-05-12 15:14:45.683687] mc_pilco.rollout > Moment-matching [state: False, cost:True], State measurement noise [policy: True, cost: False]\n",
      "[2018-05-12 15:14:50.374401] SGDOptimizer > Building computation graph for gradients\n",
      "[2018-05-12 15:14:51.303587] SGDOptimizer > Clipping gradients to norm 1.0\n",
      "[2018-05-12 15:14:51.314183] SGDOptimizer > Computing parameter update rules\n",
      "[2018-05-12 15:14:51.346138] SGDOptimizer > Compiling function for loss\n",
      "[2018-05-12 15:14:54.049504] SGDOptimizer > Compiling parameter updates\n",
      "[2018-05-12 15:15:09.986463] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_0.zip\n",
      "[2018-05-12 15:15:10.080002] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_0.zip\n",
      "[2018-05-12 15:15:10.179823] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_0.zip\n",
      "[2018-05-12 15:15:10.331773] experiment_utils > using common random numbers for dyn and pol\n",
      "[2018-05-12 15:15:10.333683] ==== Iteration [1], experience: [30 steps] ====\n",
      "[2018-05-12 15:15:10.338752] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:15:11.937195] SGDOptimizer > Initial loss [0.9777532815933228]\n",
      "\u001b[2K[2018-05-12 15:17:02.768022] SGDOptimizer > Curr loss: 4.870435E-01, n_evals: 999, Avg. time per updt: 0.109421\n",
      "[2018-05-12 15:17:02.802182] SGDOptimizer > Done training. New loss [0.700384] iter: [999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 15:17:02.805738] apply_controller > Starting run\n",
      "[2018-05-12 15:17:02.807052] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:17:02.981100] apply_controller > Done. Stopping robot. Value of run [29.993176]\n",
      "[2018-05-12 15:17:02.982710] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:17:02.984065] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:17:02.988161] train_dynamics > Dataset size:: Inputs: [ (58, 6) ], Targets: [ (58, 4) ] \n",
      "[2018-05-12 15:17:02.989722] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:17:03.023278] target_dyn_opt > Initial loss [161.2184887225258]\n",
      "\u001b[2K[2018-05-12 15:17:16.365105] target_dyn_opt > Curr loss: 1.417724E+01 [1985: 1.410008E+01], n_evals: 1999, Avg. time per updt: 0.005131\n",
      "[2018-05-12 15:17:16.375706] target_dyn_opt > Done training. New loss [14.388128] iter: [2000]\n",
      "[2018-05-12 15:17:16.378255] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:17:16.382286] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_1.zip\n",
      "[2018-05-12 15:17:16.531599] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_1.zip\n",
      "[2018-05-12 15:17:16.631783] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_1.zip\n",
      "[2018-05-12 15:17:19.723127] ==== Iteration [2], experience: [60 steps] ====\n",
      "[2018-05-12 15:17:19.730784] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:17:19.894377] SGDOptimizer > Initial loss [0.9631076455116272]\n",
      "\u001b[2K[2018-05-12 15:18:58.882599] SGDOptimizer > Curr loss: 8.796992E-01, n_evals: 999, Avg. time per updt: 0.097663\n",
      "[2018-05-12 15:18:58.912756] SGDOptimizer > Done training. New loss [0.880910] iter: [999]\n",
      "[2018-05-12 15:18:58.915281] apply_controller > Starting run\n",
      "[2018-05-12 15:18:58.916756] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:18:59.116989] apply_controller > Done. Stopping robot. Value of run [29.958811]\n",
      "[2018-05-12 15:18:59.118319] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:18:59.119752] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:18:59.122228] train_dynamics > Dataset size:: Inputs: [ (87, 6) ], Targets: [ (87, 4) ] \n",
      "[2018-05-12 15:18:59.123492] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:18:59.136564] target_dyn_opt > Initial loss [60.914604154811514]\n",
      "\u001b[2K[2018-05-12 15:19:16.509326] target_dyn_opt > Curr loss: 5.366620E+00 [1928: 5.293457E+00], n_evals: 1999, Avg. time per updt: 0.007097\n",
      "[2018-05-12 15:19:16.523346] target_dyn_opt > Done training. New loss [5.478997] iter: [2000]\n",
      "[2018-05-12 15:19:16.525914] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:19:16.528590] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_2.zip\n",
      "[2018-05-12 15:19:16.719014] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_2.zip\n",
      "[2018-05-12 15:19:16.816362] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_2.zip\n",
      "[2018-05-12 15:19:19.130816] ==== Iteration [3], experience: [90 steps] ====\n",
      "[2018-05-12 15:19:19.136045] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:19:19.283442] SGDOptimizer > Initial loss [0.9582507610321045]\n",
      "\u001b[2K[2018-05-12 15:21:10.152626] SGDOptimizer > Curr loss: 8.221866E-01, n_evals: 999, Avg. time per updt: 0.109561\n",
      "[2018-05-12 15:21:10.189775] SGDOptimizer > Done training. New loss [0.821733] iter: [999]\n",
      "[2018-05-12 15:21:10.191771] apply_controller > Starting run\n",
      "[2018-05-12 15:21:10.193009] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:21:10.394085] apply_controller > Done. Stopping robot. Value of run [26.208416]\n",
      "[2018-05-12 15:21:10.395522] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:21:10.397130] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:21:10.399406] train_dynamics > Dataset size:: Inputs: [ (116, 6) ], Targets: [ (116, 4) ] \n",
      "[2018-05-12 15:21:10.400859] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:21:10.414615] target_dyn_opt > Initial loss [11.920192623446727]\n",
      "\u001b[2K[2018-05-12 15:21:29.344378] target_dyn_opt > Curr loss: -1.897227E-02 [1975: -1.977615E-01], n_evals: 1999, Avg. time per updt: 0.007897\n",
      "[2018-05-12 15:21:29.357714] target_dyn_opt > Done training. New loss [0.046675] iter: [2000]\n",
      "[2018-05-12 15:21:29.360339] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:21:29.361936] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_3.zip\n",
      "[2018-05-12 15:21:29.610876] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_3.zip\n",
      "[2018-05-12 15:21:29.714125] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_3.zip\n",
      "[2018-05-12 15:21:32.143430] ==== Iteration [4], experience: [120 steps] ====\n",
      "[2018-05-12 15:21:32.148308] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:21:32.276560] SGDOptimizer > Initial loss [0.9004883170127869]\n",
      "\u001b[2K[2018-05-12 15:23:02.767205] SGDOptimizer > Curr loss: 6.261108E-01, n_evals: 999, Avg. time per updt: 0.089133\n",
      "[2018-05-12 15:23:02.801053] SGDOptimizer > Done training. New loss [0.621286] iter: [999]\n",
      "[2018-05-12 15:23:02.802848] apply_controller > Starting run\n",
      "[2018-05-12 15:23:02.803787] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:23:03.158807] apply_controller > Done. Stopping robot. Value of run [25.523468]\n",
      "[2018-05-12 15:23:03.160042] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:23:03.161531] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:23:03.166055] train_dynamics > Dataset size:: Inputs: [ (145, 6) ], Targets: [ (145, 4) ] \n",
      "[2018-05-12 15:23:03.167369] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:23:03.184495] target_dyn_opt > Initial loss [36.978056447875915]\n",
      "\u001b[2K[2018-05-12 15:23:22.096279] target_dyn_opt > Curr loss: -2.401841E+00 [1994: -2.914698E+00], n_evals: 1999, Avg. time per updt: 0.007881\n",
      "[2018-05-12 15:23:22.109451] target_dyn_opt > Done training. New loss [-2.513398] iter: [2000]\n",
      "[2018-05-12 15:23:22.112048] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:23:22.113788] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_4.zip\n",
      "[2018-05-12 15:23:22.407776] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_4.zip\n",
      "[2018-05-12 15:23:22.508471] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_4.zip\n",
      "[2018-05-12 15:23:24.898784] ==== Iteration [5], experience: [150 steps] ====\n",
      "[2018-05-12 15:23:24.904370] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:23:25.018727] SGDOptimizer > Initial loss [0.914828360080719]\n",
      "\u001b[2K[2018-05-12 15:25:00.985707] SGDOptimizer > Curr loss: 5.032934E-01, n_evals: 999, Avg. time per updt: 0.094482\n",
      "[2018-05-12 15:25:01.009118] SGDOptimizer > Done training. New loss [0.501989] iter: [999]\n",
      "[2018-05-12 15:25:01.013201] apply_controller > Starting run\n",
      "[2018-05-12 15:25:01.014666] apply_controller > Running for 3.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-12 15:25:01.220639] apply_controller > Done. Stopping robot. Value of run [19.265255]\n",
      "[2018-05-12 15:25:01.221863] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:25:01.223164] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:25:01.225567] train_dynamics > Dataset size:: Inputs: [ (174, 6) ], Targets: [ (174, 4) ] \n",
      "[2018-05-12 15:25:01.227149] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:25:01.240362] target_dyn_opt > Initial loss [4.656178794886873]\n",
      "\u001b[2K[2018-05-12 15:25:19.072032] target_dyn_opt > Curr loss: -4.712666E+00 [1561: -5.065705E+00], n_evals: 1999, Avg. time per updt: 0.007370\n",
      "[2018-05-12 15:25:19.084668] target_dyn_opt > Done training. New loss [-4.947109] iter: [2000]\n",
      "[2018-05-12 15:25:19.087254] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:25:19.089176] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_5.zip\n",
      "[2018-05-12 15:25:19.413800] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_5.zip\n",
      "[2018-05-12 15:25:19.509762] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_5.zip\n",
      "[2018-05-12 15:25:21.838491] ==== Iteration [6], experience: [180 steps] ====\n",
      "[2018-05-12 15:25:21.843554] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:25:21.959467] SGDOptimizer > Initial loss [0.7935174107551575]\n",
      "\u001b[2K[2018-05-12 15:26:47.018687] SGDOptimizer > Curr loss: 4.945724E-01, n_evals: 999, Avg. time per updt: 0.082730\n",
      "[2018-05-12 15:26:47.045930] SGDOptimizer > Done training. New loss [0.494316] iter: [999]\n",
      "[2018-05-12 15:26:47.049573] apply_controller > Starting run\n",
      "[2018-05-12 15:26:47.050901] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:26:47.238872] apply_controller > Done. Stopping robot. Value of run [14.420296]\n",
      "[2018-05-12 15:26:47.240224] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:26:47.241769] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:26:47.246934] train_dynamics > Dataset size:: Inputs: [ (203, 6) ], Targets: [ (203, 4) ] \n",
      "[2018-05-12 15:26:47.248996] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:26:47.285221] target_dyn_opt > Initial loss [1.9488419862509945]\n",
      "\u001b[2K[2018-05-12 15:27:07.718612] target_dyn_opt > Curr loss: -6.358322E+00 [1657: -6.604451E+00], n_evals: 1999, Avg. time per updt: 0.007755\n",
      "[2018-05-12 15:27:07.732294] target_dyn_opt > Done training. New loss [-6.597943] iter: [2000]\n",
      "[2018-05-12 15:27:07.735581] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:27:07.739744] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_6.zip\n",
      "[2018-05-12 15:27:08.137251] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_6.zip\n",
      "[2018-05-12 15:27:08.236342] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_6.zip\n",
      "[2018-05-12 15:27:10.659647] ==== Iteration [7], experience: [210 steps] ====\n",
      "[2018-05-12 15:27:10.665371] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:27:10.836813] SGDOptimizer > Initial loss [0.5229015946388245]\n",
      "\u001b[2K[2018-05-12 15:28:40.570275] SGDOptimizer > Curr loss: 4.961136E-01, n_evals: 999, Avg. time per updt: 0.088402\n",
      "[2018-05-12 15:28:40.598611] SGDOptimizer > Done training. New loss [0.488200] iter: [999]\n",
      "[2018-05-12 15:28:40.602167] apply_controller > Starting run\n",
      "[2018-05-12 15:28:40.603453] apply_controller > Running for 3.000000 seconds\n",
      "[2018-05-12 15:28:40.791098] apply_controller > Done. Stopping robot. Value of run [27.697414]\n",
      "[2018-05-12 15:28:40.792697] target_2x_mass > Stopping robot\n",
      "[2018-05-12 15:28:40.794664] train_dynamics > Training dynamics model\n",
      "[2018-05-12 15:28:40.799098] train_dynamics > Dataset size:: Inputs: [ (232, 6) ], Targets: [ (232, 4) ] \n",
      "[2018-05-12 15:28:40.800879] target_dyn_opt > Optimizing parameters via mini batches\n",
      "[2018-05-12 15:28:40.826403] target_dyn_opt > Initial loss [14.590212181338977]\n",
      "\u001b[2K[2018-05-12 15:28:59.153385] target_dyn_opt > Curr loss: -6.975416E+00 [1956: -7.578962E+00], n_evals: 1999, Avg. time per updt: 0.007615\n",
      "[2018-05-12 15:28:59.169732] target_dyn_opt > Done training. New loss [-7.248948] iter: [2000]\n",
      "[2018-05-12 15:28:59.172392] train_dynamics > Done training dynamics model\n",
      "[2018-05-12 15:28:59.174631] Experience > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/experience_7.zip\n",
      "[2018-05-12 15:28:59.624084] NNPolicy > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/policy_7.zip\n",
      "[2018-05-12 15:28:59.727093] target_dyn > Saving state to /home/juancamilog/.kusanagi/output/target_2x_mass001_no_transfer/target_2x_mass002_task_cost_from_source/target_2x_mass002_task_cost_from_source/dynamics_7.zip\n",
      "[2018-05-12 15:29:02.197360] ==== Iteration [8], experience: [240 steps] ====\n",
      "[2018-05-12 15:29:02.202258] SGDOptimizer > Optimizing parameters\n",
      "[2018-05-12 15:29:02.325757] SGDOptimizer > Initial loss [0.8927331566810608]\n",
      "\u001b[2K[2018-05-12 15:29:06.501741] SGDOptimizer > Curr loss: 4.734304E-01, n_evals: 49, Avg. time per updt: 0.083893"
     ]
    }
   ],
   "source": [
    "# experiment 2 learn starting from source policy and dynamics\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '002_task_cost_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=False,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 3 learn starting from scratch, using klqp imitation loss\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '003_il_klqp_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 4 learn starting from scratch, using klpq imitation loss\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '004_il_klpq_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 5 learn starting from source params, using klqp imitation loss\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '005_il_klqp_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 6 learn starting from source, using klpq imitation loss\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '004_il_klpq_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 0                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[0.0, 1.0], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 7 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '007_taskplusil_klqp_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 8 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '008_taskplusil_klpq_from_scratch')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=False)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=False)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 9 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '009_taskplusil_klqp_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLQP)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 8 learn starting from source params, using klqp imitation loss + task cost\n",
    "output_dir = os.path.join(utils.get_output_dir(), target_env.name + '010_taskplusil_klpq_from_source')\n",
    "utils.set_output_dir(utils.unique_path(output_dir))\n",
    "params['n_rnd'] = 1                 # number of random initial trials\n",
    "params['n_opt'] = 30                # learning iterations\n",
    "source_dyn, target_dyn = init_dyn(params, dyn_path, copy_params=True)\n",
    "source_pol, target_pol = init_pol(params, pol_path, copy_params=True)\n",
    "loss_kwargs = dict(\n",
    "    n_samples=n_samples, mm_state=True, mm_cost=True,\n",
    "    noisy_policy_input=True, crn=True, time_varying_cost=True,\n",
    "    extra_shared=extra_shared,\n",
    "    intermediate_outs=False)\n",
    "\n",
    "polopt_kwargs = dict(clip=1.0, polyak_averaging=None)\n",
    "\n",
    "setup_experiment = partial(setup_mc_pilco_experiment, pol=target_pol, dyn=target_dyn)\n",
    "\n",
    "cost = partial(task_plus_il_cost, weights=[1.0, 1e-3], loss_type=utils.ImitationLossType.KLPQ)\n",
    "\n",
    "run_pilco_experiment(\n",
    "    target_env, task_cost, setup_experiment, params,\n",
    "    loss_kwargs, polopt_kwargs,\n",
    "    minimize_cb=minimize_cb, learning_iteration_cb=learning_iteration_cb,\n",
    "    debug_plot=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

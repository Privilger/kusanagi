{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 970 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from kusanagi.ghost.algorithms.ExperienceDataset import ExperienceDataset\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "global_dtype = torch.FloatTensor\n",
    "#global_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "MATRIX_STRUCTURES = ['general', 'lower', 'upper', 'symmetric']\n",
    "class Solve(torch.autograd.Function):\n",
    "    def __init__(self, A_structure='general'):\n",
    "        self.A_structure = A_structure\n",
    "        \n",
    "    def forward(self, A, b):\n",
    "        if self.A_structure == 'lower':\n",
    "            x = torch.trtrs(b, A, False)[0]\n",
    "        elif self.A_structure == 'upper':\n",
    "            x = torch.trtrs(b, A, True)[0]\n",
    "        else:\n",
    "            x = torch.gesv(b, A)[0]\n",
    "        \n",
    "        self.save_for_backward(A, b, x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def backward(self, output_grads):\n",
    "        \"\"\"\n",
    "        Reverse-mode gradient updates for matrix solve operation c = A \\ b.\n",
    "        Symbolic expression for updates taken from [1]_.\n",
    "        References\n",
    "        ----------\n",
    "        ..[1] M. B. Giles, \"An extended collection of matrix derivative results\n",
    "          for forward and reverse mode automatic differentiation\",\n",
    "          http://eprints.maths.ox.ac.uk/1079/\n",
    "        \"\"\"\n",
    "        A, b, x = self.saved_tensors\n",
    "        \n",
    "        grad_x = output_grads\n",
    "       \n",
    "        # transpose solve\n",
    "        if self.A_structure == 'lower':\n",
    "            grad_b = torch.trtrs(b, A.t(), True)[0]\n",
    "        elif self.A_structure == 'upper':\n",
    "            grad_b = torch.trtrs(b, A.t(), False)[0]\n",
    "        else:\n",
    "            grad_b = torch.gesv(b, A.t())[0]\n",
    "            \n",
    "        # force outer product if vector second input\n",
    "        grad_A = -grad_b[:,None].mm(x[None,:])\\\n",
    "                 if x.ndimension() == 1\\\n",
    "                 else -grad_b.mm(x.t())\n",
    "                \n",
    "        if self.A_structure == 'lower':\n",
    "            grad_A = torch.tril(grad_A)\n",
    "        elif self.A_structure == 'upper':\n",
    "            grad_A = torch.triu(grad_A)\n",
    "        \n",
    "        return grad_A, grad_b\n",
    "    \n",
    "def solve_lower_triangular(A, b):\n",
    "    return Solve(A_structure='lower')(A, b)\n",
    "\n",
    "def solve_upper_triangular(A, b):\n",
    "    return Solve(A_structure='upper')(A, b)\n",
    "\n",
    "class Cholesky(torch.autograd.Function):\n",
    "    def __init__(self, lower=True):\n",
    "        self.lower = lower\n",
    "        \n",
    "    def forward(self, input):\n",
    "        chol = torch.potrf(input, not self.lower)\n",
    "        self.save_for_backward(input,chol)\n",
    "        return chol\n",
    "    \n",
    "    def backward(self, output_grads):\n",
    "        \"\"\"\n",
    "        Cholesky decomposition reverse-mode gradient update.\n",
    "        Symbolic expression for reverse-mode Cholesky gradient taken from [0]_\n",
    "        References\n",
    "        ----------\n",
    "        .. [0] I. Murray, \"Differentiation of the Cholesky decomposition\",\n",
    "           http://arxiv.org/abs/1602.07527\n",
    "        \"\"\"\n",
    "\n",
    "        x, chol_x = self.saved_tensors\n",
    "        dz = output_grads\n",
    "        \n",
    "        # TODO deal with nans \n",
    "        \n",
    "        if not self.lower:\n",
    "            chol_x = chol_x.t()\n",
    "            dz = dz.t()\n",
    "\n",
    "        def tril_and_halve_diagonal(mtx):\n",
    "            \"\"\"Extracts lower triangle of square matrix and halves diagonal.\"\"\"\n",
    "            return torch.tril(mtx) - torch.diag(torch.diag(mtx)/2)\n",
    "            \n",
    "        def conjugate_solve_triangular(outer, inner):\n",
    "            \"\"\"Computes L^{-T} P L^{-1} for lower-triangular L.\"\"\"\n",
    "            return solve_upper_triangular(\n",
    "                outer.t(), solve_upper_triangular(outer.t(), inner.t()).t())\n",
    "        \n",
    "     \n",
    "        s = conjugate_solve_triangular(\n",
    "            chol_x, tril_and_halve_diagonal(chol_x.t().mm(dz)))\n",
    "        \n",
    "        s2 = s + s.t()\n",
    "        sdiag = torch.diag(torch.diag(s))\n",
    "        \n",
    "        if self.lower:\n",
    "            grad = torch.tril(s2) - sdiag\n",
    "        else:\n",
    "            grad = torch.triu(s2) - sdiag\n",
    "            \n",
    "        return grad\n",
    "    \n",
    "def cholesky(input, lower=True):\n",
    "    return Cholesky(lower)(input)\n",
    "\n",
    "def maha(X,Y,M=None):\n",
    "    if not M is None:\n",
    "        XM = X.mm(M)\n",
    "        YM = Y.mm(M)\n",
    "        dist = (XM*X).sum(1).repeat(1, Y.size(0))\\\n",
    "             + (YM*Y).sum(1).t().repeat(X.size(0), 1)\\\n",
    "             - 2*XM.mm(Y.t())\n",
    "    else:\n",
    "        dist = (X**2).sum(1).repeat(1,Y.size(0))\\\n",
    "             + (Y**2).sum(1).t().repeat(X.size(0),1)\\\n",
    "             - 2*X.mm(Y.t())\n",
    "    return dist\n",
    "\n",
    "def SEard(loghyp, X, Y):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    n, D = X.size()\n",
    "    dist = maha(X, Y, torch.diag(torch.exp(-2*loghyp[:D])))\n",
    "    return torch.exp(2*loghyp[D].expand(dist.size()) - 0.5*dist )\n",
    "\n",
    "def Noise(loghyp, X, Y=None):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    if X is Y:\n",
    "        ones = torch.autograd.Variable(torch.ones(X.size(0)))\n",
    "        K = ones*torch.exp(2*loghyp).expand(ones.size())\n",
    "        return K.diag()\n",
    "    else:\n",
    "        return torch.zeros(X.size(0), X.size(0))\n",
    "    \n",
    "def Sum(loghyp_list,cov_list, X, Y=None):\n",
    "    D = len(cov_list)\n",
    "    K = sum([cov_list[i](loghyp_list[i], X, Y) for i in range(D)])\n",
    "    return K\n",
    "\n",
    "def SEard_params(X, Y):\n",
    "    n, idims = X.size()\n",
    "    odims = Y.size(1)\n",
    "    \n",
    "    loghyp = torch.autograd.Variable(torch.Tensor(odims,idims+1).type(global_dtype),\n",
    "                                     requires_grad=True)\n",
    "    loghyp.data[:, :idims] = 0.5*X.std(0).repeat(odims,1)\n",
    "    loghyp.data[:, idims] = 0.5*Y.std(0)\n",
    "    \n",
    "    return loghyp\n",
    "\n",
    "def Noise_params(X, Y):\n",
    "    n, idims = X.size()\n",
    "    odims = Y.size(1)\n",
    "    \n",
    "    loghyp = torch.autograd.Variable(torch.Tensor(odims,1), requires_grad=True)\n",
    "    loghyp.data[:, 0] = 0.1*Y.std(0)\n",
    "    \n",
    "    return loghyp\n",
    "\n",
    "def GP_loss(loghyps, covs, X, Y):\n",
    "    if not type(covs) is list:\n",
    "        covs= covs[covs]\n",
    "        \n",
    "    n, idims = X.size()\n",
    "    odims = Y.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    X_ = torch.autograd.Variable(X)\n",
    "    loghyps_ = zip(*loghyps)\n",
    "    \n",
    "    for i in range(odims):\n",
    "        K = Sum(loghyps_[i],covs, X_)\n",
    "        L = cholesky(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-06-16 17:11:33.952255] Experience > Loading state from c:/Users/jcgamboahiguera/kusanagi/examples/learned_policies/cartpole/PILCO_SSGP_UI_Cartpole_RBFPolicy_sat_dataset.zip\n"
     ]
    }
   ],
   "source": [
    "exp = ExperienceDataset(filename='c:/Users/jcgamboahiguera/kusanagi/examples/learned_policies/cartpole/PILCO_SSGP_UI_Cartpole_RBFPolicy_sat_dataset')\n",
    "X, Y = exp.get_dynmodel_dataset()\n",
    "#convert to torch tensors\n",
    "X, Y = torch.from_numpy(X).type(global_dtype),torch.from_numpy(Y).type(global_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = torch.autograd.Variable(X)\n",
    "loghyps = SEard_params(X, Y), Noise_params(X,Y)\n",
    "K = Sum(list(zip(*loghyps))[0],[SEard,Noise], X_, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GP_loss(loghyps, [SEard, Noise], X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 800])\n"
     ]
    }
   ],
   "source": [
    "cholK = cholesky(K)\n",
    "print(cholK.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.randn(800, 800)\n",
    "S = S + S.t()\n",
    "S = torch.autograd.Variable(S, requires_grad=True)\n",
    "Y0 = torch.autograd.Variable(Y[:,0,None], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.0057\n",
       " -0.0556\n",
       "  0.3713\n",
       " -0.2362\n",
       " -0.4194\n",
       "  0.1312\n",
       " -0.2695\n",
       "  0.3175\n",
       "  0.4956\n",
       " -0.0331\n",
       " -0.3058\n",
       "  0.1665\n",
       " -0.0740\n",
       " -0.2443\n",
       " -0.0427\n",
       "  0.4696\n",
       " -0.1898\n",
       " -0.1400\n",
       " -0.2392\n",
       " -0.4488\n",
       " -0.3576\n",
       "  0.3106\n",
       "  0.2348\n",
       "  0.6964\n",
       " -0.4666\n",
       "  0.5703\n",
       " -0.1438\n",
       " -0.2647\n",
       " -0.0453\n",
       "  0.1285\n",
       "  0.2474\n",
       "  0.0786\n",
       " -0.1980\n",
       "  0.4264\n",
       "  0.4965\n",
       "  0.0320\n",
       " -0.7209\n",
       "  0.1076\n",
       "  0.1872\n",
       " -0.0288\n",
       " -0.4626\n",
       "  0.1256\n",
       "  0.2427\n",
       "  0.1278\n",
       " -0.2071\n",
       "  0.1853\n",
       "  0.2104\n",
       " -0.4828\n",
       "  0.2470\n",
       " -0.0944\n",
       " -0.6974\n",
       " -0.0365\n",
       " -0.3069\n",
       " -0.1974\n",
       " -0.2941\n",
       "  0.2646\n",
       "  0.1484\n",
       "  0.2164\n",
       "  0.3965\n",
       " -0.3464\n",
       "  0.0524\n",
       "  0.2672\n",
       "  0.0288\n",
       " -0.1876\n",
       "  0.0779\n",
       "  0.1376\n",
       " -0.3034\n",
       "  0.1568\n",
       "  0.1350\n",
       " -0.1886\n",
       "  0.1813\n",
       "  0.3014\n",
       " -0.2159\n",
       "  0.5240\n",
       "  0.1754\n",
       "  0.1902\n",
       " -0.2548\n",
       "  0.3521\n",
       "  0.0205\n",
       "  0.0207\n",
       "  0.4075\n",
       " -0.6382\n",
       "  0.3337\n",
       "  0.3772\n",
       " -0.0833\n",
       "  0.0847\n",
       " -0.1878\n",
       " -0.0394\n",
       " -0.3982\n",
       " -0.1745\n",
       "  0.0888\n",
       "  0.2839\n",
       "  0.2704\n",
       " -0.1230\n",
       "  0.5504\n",
       "  0.1915\n",
       " -0.3505\n",
       " -0.1432\n",
       " -0.0497\n",
       " -0.1323\n",
       " -0.1091\n",
       " -0.1909\n",
       "  0.2567\n",
       "  0.0568\n",
       "  0.3376\n",
       "  0.2368\n",
       "  0.6110\n",
       " -0.0106\n",
       " -0.8380\n",
       "  0.1014\n",
       "  0.4706\n",
       "  0.1073\n",
       " -0.0483\n",
       " -0.0691\n",
       " -0.5911\n",
       " -0.0804\n",
       " -0.4266\n",
       "  0.0355\n",
       "  0.1357\n",
       " -0.0542\n",
       " -0.2197\n",
       " -0.4768\n",
       " -0.3418\n",
       "  0.3415\n",
       "  0.2369\n",
       " -0.2908\n",
       "  0.0867\n",
       "  0.6965\n",
       "  0.1199\n",
       " -0.0404\n",
       " -0.3572\n",
       " -0.0005\n",
       "  0.1727\n",
       "  0.4330\n",
       " -0.1619\n",
       "  0.0104\n",
       "  0.3469\n",
       "  0.1466\n",
       "  0.0975\n",
       "  0.6494\n",
       " -0.1306\n",
       "  0.2148\n",
       "  0.1264\n",
       "  0.1198\n",
       "  0.3015\n",
       "  0.1643\n",
       "  0.1805\n",
       " -0.1533\n",
       "  0.5874\n",
       "  0.3390\n",
       "  0.0817\n",
       "  0.0469\n",
       " -0.1123\n",
       "  0.0470\n",
       " -0.1294\n",
       "  0.5373\n",
       "  0.2062\n",
       "  0.5017\n",
       "  0.5708\n",
       "  0.0380\n",
       " -0.1381\n",
       "  0.2858\n",
       "  0.0846\n",
       " -0.2708\n",
       "  0.6019\n",
       " -0.3995\n",
       "  0.5467\n",
       "  0.1110\n",
       "  0.3166\n",
       "  0.2751\n",
       "  0.0817\n",
       " -0.2833\n",
       " -0.3946\n",
       "  0.1813\n",
       "  0.5367\n",
       " -0.6785\n",
       "  0.0400\n",
       " -0.3032\n",
       "  0.2197\n",
       "  0.1271\n",
       "  0.0526\n",
       "  0.4823\n",
       " -0.0116\n",
       " -0.1641\n",
       "  0.1064\n",
       "  0.2177\n",
       "  0.0537\n",
       "  0.2791\n",
       "  0.1816\n",
       " -0.0397\n",
       "  0.7343\n",
       " -0.0813\n",
       "  0.2605\n",
       "  0.1102\n",
       " -0.0108\n",
       " -0.0615\n",
       " -0.0093\n",
       "  0.2219\n",
       " -0.1290\n",
       " -0.8114\n",
       "  0.1782\n",
       "  0.0548\n",
       " -0.3511\n",
       "  0.0616\n",
       " -0.1191\n",
       "  0.2651\n",
       " -0.1747\n",
       " -0.3948\n",
       "  0.0618\n",
       "  0.2104\n",
       "  0.3327\n",
       " -0.1270\n",
       " -0.3321\n",
       "  0.2100\n",
       " -0.1684\n",
       " -0.7399\n",
       " -0.6491\n",
       " -0.2023\n",
       " -0.2210\n",
       "  0.0005\n",
       "  0.0387\n",
       " -0.1121\n",
       "  0.4152\n",
       "  0.0509\n",
       " -0.2139\n",
       "  0.0882\n",
       "  0.3621\n",
       "  0.0476\n",
       "  0.2113\n",
       "  0.2681\n",
       " -0.1854\n",
       "  0.0543\n",
       "  0.2606\n",
       "  0.3739\n",
       " -0.2459\n",
       " -0.0467\n",
       "  0.5304\n",
       "  0.1355\n",
       " -0.6194\n",
       "  0.3097\n",
       "  0.1807\n",
       " -0.4984\n",
       " -0.5401\n",
       " -0.0941\n",
       " -0.4037\n",
       " -0.0639\n",
       " -0.1785\n",
       " -0.0419\n",
       "  0.1310\n",
       "  0.0987\n",
       "  0.0618\n",
       " -0.3885\n",
       "  0.3066\n",
       "  0.0350\n",
       "  0.4953\n",
       " -0.0509\n",
       "  0.1360\n",
       "  0.3090\n",
       " -0.0582\n",
       " -0.1520\n",
       "  0.3269\n",
       " -0.1971\n",
       " -0.2649\n",
       "  0.2960\n",
       " -0.2529\n",
       "  0.2603\n",
       "  0.1069\n",
       " -0.0363\n",
       " -0.0549\n",
       "  0.0002\n",
       "  0.0738\n",
       "  0.2442\n",
       "  0.2255\n",
       "  0.0262\n",
       " -0.3632\n",
       "  0.0466\n",
       "  0.1116\n",
       " -0.0269\n",
       " -0.0920\n",
       " -0.2157\n",
       "  0.2332\n",
       "  0.1710\n",
       " -0.4300\n",
       "  0.1244\n",
       "  0.0249\n",
       "  0.1507\n",
       "  0.1565\n",
       " -0.0229\n",
       "  0.1754\n",
       "  0.2413\n",
       " -0.4924\n",
       " -0.1788\n",
       " -0.7223\n",
       "  0.3651\n",
       "  0.3290\n",
       " -0.3044\n",
       "  0.3222\n",
       " -0.4366\n",
       "  0.3173\n",
       "  0.0415\n",
       " -0.1705\n",
       " -0.0290\n",
       " -0.2709\n",
       " -0.0584\n",
       " -0.5497\n",
       " -0.0281\n",
       "  0.1468\n",
       " -0.3744\n",
       " -0.5118\n",
       "  0.3208\n",
       " -0.1768\n",
       "  0.0385\n",
       "  0.5429\n",
       " -0.1581\n",
       " -0.3950\n",
       "  0.7749\n",
       "  0.4135\n",
       " -0.2168\n",
       " -0.0859\n",
       "  0.3646\n",
       " -0.3537\n",
       " -0.5135\n",
       " -0.1148\n",
       " -0.1827\n",
       "  0.3283\n",
       " -0.1665\n",
       " -0.4565\n",
       "  0.0129\n",
       " -0.1494\n",
       "  0.0287\n",
       "  0.0251\n",
       " -0.0970\n",
       "  0.1064\n",
       " -0.4040\n",
       "  0.0229\n",
       " -0.5298\n",
       " -0.0434\n",
       "  0.1200\n",
       " -0.0857\n",
       " -0.3835\n",
       "  0.1497\n",
       " -0.3882\n",
       "  0.1086\n",
       " -0.1890\n",
       " -0.0652\n",
       " -0.3063\n",
       "  0.5594\n",
       "  0.7557\n",
       "  0.3799\n",
       " -0.2789\n",
       "  0.6696\n",
       " -0.2803\n",
       "  0.2967\n",
       "  0.2165\n",
       "  0.0295\n",
       " -0.8781\n",
       " -0.0610\n",
       " -0.4232\n",
       "  0.0209\n",
       " -0.0408\n",
       "  0.3536\n",
       " -0.0307\n",
       "  0.1195\n",
       " -0.1656\n",
       "  0.1833\n",
       " -0.5773\n",
       " -0.0166\n",
       " -0.1489\n",
       " -0.1594\n",
       " -0.0306\n",
       " -0.0043\n",
       "  0.2392\n",
       " -0.0518\n",
       " -0.1240\n",
       " -0.3042\n",
       "  0.3692\n",
       "  0.0134\n",
       " -0.0960\n",
       " -0.1215\n",
       "  0.7320\n",
       "  0.4159\n",
       " -0.0871\n",
       " -0.5411\n",
       "  0.3073\n",
       " -0.4268\n",
       "  0.4066\n",
       "  0.2662\n",
       " -0.1207\n",
       "  0.3013\n",
       " -0.1575\n",
       "  0.4620\n",
       "  0.0010\n",
       " -0.1015\n",
       " -0.2130\n",
       " -0.1746\n",
       "  0.0038\n",
       " -0.1062\n",
       " -0.5678\n",
       "  0.3455\n",
       " -0.4067\n",
       " -0.2117\n",
       " -0.0418\n",
       "  0.3836\n",
       " -0.2424\n",
       " -0.1913\n",
       "  0.4241\n",
       "  0.2610\n",
       " -0.2871\n",
       " -0.3290\n",
       " -0.2036\n",
       " -0.1242\n",
       " -0.2140\n",
       " -0.3558\n",
       "  0.0333\n",
       "  0.0377\n",
       " -0.2734\n",
       "  0.4379\n",
       "  0.2170\n",
       "  0.6341\n",
       "  0.2219\n",
       "  0.1313\n",
       " -0.1130\n",
       "  0.3492\n",
       " -0.1817\n",
       " -0.6027\n",
       "  0.0688\n",
       "  0.2446\n",
       " -0.1440\n",
       " -0.0355\n",
       "  0.1011\n",
       "  0.2310\n",
       " -0.3683\n",
       " -0.2051\n",
       "  0.2877\n",
       " -0.0295\n",
       " -0.3814\n",
       "  0.0059\n",
       "  0.1633\n",
       "  0.3182\n",
       "  0.3876\n",
       "  0.4660\n",
       "  0.2277\n",
       " -0.5618\n",
       " -0.5157\n",
       "  0.4065\n",
       "  0.7295\n",
       "  0.0047\n",
       "  0.0502\n",
       " -0.4211\n",
       " -0.1294\n",
       " -0.0137\n",
       " -0.0281\n",
       "  0.1413\n",
       "  0.2436\n",
       "  0.2292\n",
       " -0.3439\n",
       " -0.1363\n",
       " -0.3331\n",
       "  0.0622\n",
       " -0.2900\n",
       " -0.2941\n",
       "  0.0617\n",
       " -0.2388\n",
       " -0.0588\n",
       "  0.2282\n",
       "  0.2423\n",
       "  0.5182\n",
       "  0.7088\n",
       " -0.2651\n",
       " -0.1046\n",
       " -0.2013\n",
       "  0.4356\n",
       " -0.3303\n",
       " -0.1512\n",
       " -0.0846\n",
       " -0.1294\n",
       "  0.0731\n",
       "  0.2222\n",
       "  0.2764\n",
       "  0.4109\n",
       " -0.3008\n",
       " -0.7839\n",
       " -0.1051\n",
       " -0.3336\n",
       "  0.5060\n",
       " -0.3184\n",
       " -0.0407\n",
       " -0.2262\n",
       " -0.2458\n",
       "  0.0885\n",
       "  0.4889\n",
       "  0.1120\n",
       " -0.4527\n",
       "  0.1898\n",
       " -0.2931\n",
       "  0.3258\n",
       " -0.2558\n",
       " -0.3230\n",
       "  0.0889\n",
       " -0.2870\n",
       "  0.4339\n",
       "  0.3940\n",
       " -0.7839\n",
       " -0.0353\n",
       "  0.6031\n",
       " -0.2312\n",
       " -0.2356\n",
       " -0.1984\n",
       " -0.0785\n",
       "  0.0509\n",
       " -0.3270\n",
       "  0.1051\n",
       " -0.5027\n",
       " -0.4472\n",
       " -0.1240\n",
       " -0.2111\n",
       " -0.3460\n",
       " -0.0733\n",
       "  0.1606\n",
       "  0.0876\n",
       "  0.0901\n",
       " -0.0599\n",
       "  0.2317\n",
       "  0.2816\n",
       " -0.0193\n",
       " -0.0923\n",
       "  0.7224\n",
       " -0.2949\n",
       " -0.1362\n",
       "  0.1851\n",
       "  0.0717\n",
       " -0.6008\n",
       "  0.2796\n",
       "  0.5344\n",
       "  0.2172\n",
       " -0.1236\n",
       "  0.3155\n",
       " -0.1326\n",
       "  0.0218\n",
       " -0.0469\n",
       " -0.0839\n",
       "  0.2693\n",
       "  0.0733\n",
       " -0.2288\n",
       "  0.2491\n",
       " -0.1583\n",
       "  0.1758\n",
       "  0.2092\n",
       " -0.1537\n",
       " -0.0200\n",
       " -0.2583\n",
       " -0.0768\n",
       "  0.4726\n",
       " -0.0513\n",
       "  0.0566\n",
       " -0.1579\n",
       " -0.0878\n",
       " -0.2580\n",
       "  0.1624\n",
       "  0.3177\n",
       " -0.1426\n",
       "  0.1779\n",
       " -0.2104\n",
       " -0.0873\n",
       " -0.0976\n",
       " -0.2689\n",
       " -0.3062\n",
       " -0.0638\n",
       "  0.6315\n",
       " -0.8094\n",
       " -0.3944\n",
       " -0.1441\n",
       " -0.0765\n",
       " -0.0859\n",
       " -0.1119\n",
       " -0.1194\n",
       " -0.0397\n",
       " -0.3192\n",
       "  0.0058\n",
       " -0.2512\n",
       " -0.4194\n",
       " -0.1790\n",
       " -0.3210\n",
       " -0.4613\n",
       "  0.0464\n",
       " -0.4372\n",
       "  0.2191\n",
       " -0.4257\n",
       " -0.0696\n",
       " -0.0138\n",
       " -0.3227\n",
       " -0.2699\n",
       " -0.0688\n",
       " -0.2379\n",
       " -0.3181\n",
       " -0.1430\n",
       "  0.3330\n",
       " -0.0391\n",
       " -0.1859\n",
       " -0.0591\n",
       " -0.1233\n",
       "  0.3043\n",
       "  0.0906\n",
       " -0.1328\n",
       " -0.2411\n",
       " -0.2608\n",
       "  0.0259\n",
       " -0.3979\n",
       " -0.0937\n",
       " -0.1103\n",
       " -0.1341\n",
       "  0.2301\n",
       " -0.0222\n",
       " -0.2476\n",
       "  0.2720\n",
       "  0.4298\n",
       "  0.2745\n",
       "  0.2265\n",
       "  0.1295\n",
       "  0.6581\n",
       "  0.4511\n",
       " -0.3537\n",
       "  0.2405\n",
       " -0.5387\n",
       "  0.3613\n",
       "  0.1043\n",
       " -0.6244\n",
       " -0.3951\n",
       "  0.2379\n",
       "  0.0100\n",
       " -0.4179\n",
       "  0.1227\n",
       " -0.0047\n",
       " -0.0413\n",
       "  0.0657\n",
       " -0.1130\n",
       "  0.2213\n",
       "  0.4107\n",
       " -0.2537\n",
       "  0.1147\n",
       " -0.0477\n",
       "  0.4040\n",
       " -0.6817\n",
       "  0.2597\n",
       " -0.1534\n",
       "  0.1198\n",
       " -0.0636\n",
       "  0.4681\n",
       "  0.1775\n",
       " -0.2141\n",
       "  0.1257\n",
       "  0.8117\n",
       " -0.0709\n",
       "  0.4782\n",
       "  0.2135\n",
       "  0.0527\n",
       "  0.1234\n",
       "  0.0437\n",
       "  0.1792\n",
       " -0.4472\n",
       "  0.2042\n",
       " -0.0235\n",
       " -0.2716\n",
       "  0.4801\n",
       "  0.1635\n",
       "  0.4076\n",
       " -0.0651\n",
       " -0.2470\n",
       "  0.0065\n",
       " -0.3778\n",
       "  0.6496\n",
       "  0.2522\n",
       "  0.3569\n",
       "  0.2996\n",
       "  0.1372\n",
       "  0.2813\n",
       " -0.4286\n",
       " -0.1597\n",
       " -0.2173\n",
       " -0.5359\n",
       "  0.3232\n",
       " -0.1434\n",
       "  0.6950\n",
       "  0.2598\n",
       " -0.1677\n",
       "  0.2052\n",
       " -0.3913\n",
       "  0.0825\n",
       " -0.2407\n",
       " -0.0204\n",
       " -0.1225\n",
       "  0.4581\n",
       "  0.4244\n",
       "  0.3304\n",
       "  0.0546\n",
       " -0.5191\n",
       " -0.0603\n",
       " -0.0210\n",
       "  0.5287\n",
       " -0.2121\n",
       "  0.0639\n",
       "  0.3636\n",
       " -0.0191\n",
       " -0.2894\n",
       "  0.6665\n",
       " -0.0168\n",
       "  0.0611\n",
       " -0.1263\n",
       " -0.1094\n",
       "  0.1265\n",
       "  0.1363\n",
       " -0.1857\n",
       "  0.1639\n",
       " -0.0690\n",
       " -0.6171\n",
       " -0.0017\n",
       "  0.7628\n",
       "  0.2885\n",
       "  0.2997\n",
       "  0.6449\n",
       " -0.5995\n",
       "  0.4098\n",
       "  0.6158\n",
       " -0.5131\n",
       " -0.0596\n",
       "  0.2705\n",
       "  0.0288\n",
       " -0.4657\n",
       "  0.1195\n",
       " -0.8232\n",
       " -0.2004\n",
       "  0.0775\n",
       " -0.4137\n",
       "  0.7612\n",
       " -0.1362\n",
       "  0.5165\n",
       " -0.0526\n",
       "  0.1356\n",
       "  0.0733\n",
       " -0.0149\n",
       " -0.2002\n",
       "  0.0009\n",
       "  0.2220\n",
       " -0.0442\n",
       "  0.2702\n",
       " -0.2130\n",
       "  0.4534\n",
       " -0.4641\n",
       "  0.1697\n",
       " -0.2536\n",
       "  0.6722\n",
       " -0.3257\n",
       " -0.0859\n",
       "  0.0273\n",
       " -0.0095\n",
       " -0.1712\n",
       "  0.5812\n",
       "  0.7121\n",
       "  0.1706\n",
       " -0.3886\n",
       "  0.1308\n",
       "  0.3170\n",
       " -0.2627\n",
       " -0.8287\n",
       " -0.4672\n",
       " -0.5738\n",
       "  0.3114\n",
       "  0.8552\n",
       " -0.1448\n",
       " -0.2181\n",
       "  0.2966\n",
       "  0.3463\n",
       " -0.3762\n",
       "  0.6988\n",
       "  0.0061\n",
       "  0.3962\n",
       " -0.2000\n",
       " -0.0994\n",
       " -0.2158\n",
       " -0.1573\n",
       "  0.0335\n",
       " -0.1080\n",
       " -0.3381\n",
       " -0.0334\n",
       " -0.5002\n",
       " -0.0316\n",
       " -0.0903\n",
       "  0.0884\n",
       " -0.6090\n",
       " -0.2749\n",
       " -0.0747\n",
       " -0.0283\n",
       "  0.0144\n",
       "  0.0903\n",
       " -0.1505\n",
       "  0.3159\n",
       " -0.1771\n",
       "  0.2604\n",
       "  0.0681\n",
       " -0.1887\n",
       " [torch.FloatTensor of size 800x1],)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = Solve('general')(S, Y0)\n",
    "test_ = torch.eye(R.size(0)).view(R.size(0),1,R.size(0))\n",
    "def vjp():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "# inti vars\n",
    "s = theano.tensor.matrix('s')\n",
    "y = theano.tensor.vector('y')\n",
    "s_ = theano.tensor.slinalg.cholesky(s+1e2*theano.tensor.eye(s.shape[0]))\n",
    "r = theano.tensor.slinalg.Solve()(s_, y)\n",
    "\n",
    "# jacobian using scan\n",
    "dr1 = theano.tensor.jacobian(r, s)\n",
    "\n",
    "dummy_t = theano.tensor.ones((s.shape[0],))\n",
    "dummy = theano.tensor.Lop(r, s, dummy_t)\n",
    "\n",
    "from theano.scan_module.scan_utils import clone as t_clone\n",
    "from theano.scan_module.scan_utils import map_variables as t_map\n",
    "\n",
    "# jacobian using Lop and identity matrix\n",
    "dr2, dr2_updts = theano.map(lambda t, r ,s: theano.tensor.Lop(r, s, t),\n",
    "                            sequences=theano.tensor.eye(s.shape[0]),\n",
    "                            non_sequences=[r, s])\n",
    "\n",
    "dr3, dr3_updts = theano.map(lambda t, r ,s: theano.clone(dummy, replace={dummy_t: t}),\n",
    "                            sequences=theano.tensor.eye(s.shape[0]),\n",
    "                            non_sequences=[r, s])\n",
    "\n",
    "# do it by replicating inputs\n",
    "inp_list = [(s_.clone(), y.clone()) for i in range(r.shape[0])\n",
    "r_list = [theano.tensor.slinalg.Solve()(si, yi) for si, yi in inp_list]\n",
    "\n",
    "# compile funcs\n",
    "f = theano.function(outputs=[r], inputs=[s, y])\n",
    "df1 = theano.function(outputs=dr1, inputs=[s, y])\n",
    "df2 = theano.function(outputs=dr2, inputs=[s, y], updates=dr2_updts)\n",
    "df3 = theano.function(outputs=dr3, inputs=[s, y], updates=dr3_updts)\n",
    "\n",
    "# test\n",
    "#D = 200\n",
    "#S = np.random.randn(D, D).astype(theano.config.floatX)\n",
    "#S = S + S.T\n",
    "#Y = np.ones(D).astype(theano.config.floatX)\n",
    "R1 = df1(S.data[:100,:100].numpy(), Y.numpy()[:100,0])\n",
    "R2 = df2(S.data[:100,:100].numpy(), Y.numpy()[:100,0])\n",
    "R3 = df3(S.data[:100,:100].numpy(), Y.numpy()[:100,0])\n",
    "#np.allclose(R1, R2.reshape(R1.shape), atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(R2, R3.reshape(R1.shape), atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 100, 100), (100, 100, 100))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1.shape, R2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 ms ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df1(S.data[:100,:100].numpy(), Y.numpy()[:100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 ms ± 25.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df2(S.data[:100,:100].numpy(), Y.numpy()[:100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 ms ± 8.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df3(S.data[:100,:100].numpy(), Y.numpy()[:100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s, y]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.gof.graph.inputs([dummy],blockers=[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

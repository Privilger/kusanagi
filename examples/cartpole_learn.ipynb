{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " %matplotlib tk\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "from kusanagi.ghost import control\n",
    "from kusanagi.ghost import regression\n",
    "from kusanagi.shell import cartpole\n",
    "from kusanagi.ghost.algorithms import pilco, mc_pilco\n",
    "from kusanagi.ghost.optimizers import ScipyOptimizer, SGDOptimizer\n",
    "from kusanagi.base import apply_controller, train_dynamics, ExperienceDataset\n",
    "from kusanagi import utils\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# np.random.seed(1337)\n",
    "np.set_printoptions(linewidth=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rollout(rollout_fn, *args, **kwargs):\n",
    "    fig = kwargs.get('fig')\n",
    "    axarr = kwargs.get('axarr')\n",
    "    loss, costs, trajectories = rollout_fn(*args)\n",
    "    n_samples, T, dims = trajectories.shape\n",
    "\n",
    "    if fig is None or axarr is None:\n",
    "        fig, axarr = plt.subplots(dims, sharex=True)\n",
    "    exp_states = np.array(exp.states)\n",
    "    for d in range(dims):\n",
    "        axarr[d].clear()\n",
    "        st = trajectories[:, :, d]\n",
    "        # plot predictive distribution\n",
    "        for i in range(n_samples):\n",
    "            axarr[d].plot(\n",
    "                np.arange(T-1), st[i, :-1], color='steelblue', alpha=0.3)\n",
    "        # for i in range(len(exp.states)):\n",
    "        #    axarr[d].plot(\n",
    "        #         np.arange(T-1), exp_states[i,1:,d],\n",
    "        #         color='orange', alpha=0.3)\n",
    "        # plot experience\n",
    "        axarr[d].plot(\n",
    "            np.arange(T-1), np.array(exp.states[-1])[1:H, d], color='red')\n",
    "        axarr[d].plot(\n",
    "            np.arange(T-1), st[:, :-1].mean(0), color='purple')\n",
    "    plt.show(block=False)\n",
    "    plt.waitforbuttonpress(0.1)\n",
    "\n",
    "    return fig, axarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_bnn_dyn = True\n",
    "use_bnn_pol = True\n",
    "\n",
    "# setup output directory\n",
    "utils.set_output_dir(os.path.join(utils.get_output_dir(), 'cartpole'))\n",
    "\n",
    "params = cartpole.default_params()\n",
    "n_rnd = 4                           # number of random initial trials\n",
    "n_opt = 100                         # learning iterations\n",
    "n_samples = 100                      # number of MC samples if bayesian nn\n",
    "learning_rate = 1e-3\n",
    "polyak_averaging = 0.99\n",
    "H = params['max_steps']\n",
    "gamma = params['discount']\n",
    "angle_dims = params['angle_dims']\n",
    "\n",
    "# initial state distribution\n",
    "p0 = params['state0_dist']\n",
    "D = p0.mean.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init environment\n",
    "env = cartpole.Cartpole(**params['plant'])\n",
    "\n",
    "# init policy\n",
    "pol = control.NNPolicy(p0.mean.size, **params['policy'])\\\n",
    "    if use_bnn_pol else control.RBFPolicy(**params['policy'])\n",
    "randpol = control.RandPolicy(maxU=pol.maxU)\n",
    "\n",
    "# init dynmodel\n",
    "dyn = regression.BNN(**params['dynamics_model'])\\\n",
    "    if use_bnn_dyn else regression.SSGP_UI(**params['dynamics_model'])\n",
    "\n",
    "# init cost model\n",
    "cost = partial(cartpole.cartpole_loss, **params['cost'])\n",
    "\n",
    "# create experience dataset\n",
    "exp = ExperienceDataset()\n",
    "\n",
    "# init policy optimizer\n",
    "if use_bnn_dyn:\n",
    "    params['optimizer']['min_method'] = 'adam'\n",
    "    params['optimizer']['max_evals'] = 1000\n",
    "    polopt = SGDOptimizer(**params['optimizer'])\n",
    "else:\n",
    "    polopt = ScipyOptimizer(**params['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# callback executed after every call to env.step\n",
    "def step_cb(state, action, cost, info):\n",
    "    exp.add_sample(state, action, cost, info)\n",
    "    env.render()\n",
    "\n",
    "def polopt_cb(*args, **kwargs):\n",
    "    if hasattr(dyn, 'update'):\n",
    "        dyn.update()\n",
    "    if hasattr(pol, 'update'):\n",
    "        pol.update()\n",
    "        \n",
    "    loss, dloss = args[:2]\n",
    "    grad_norms = [np.sqrt((d.__array__()**2).sum()) for d in dloss]\n",
    "    if sum(grad_norms) >  10.0*len(grad_norms):\n",
    "        print([np.sqrt((d.__array__()**2).sum()) for d in dloss])\n",
    "\n",
    "# function to execute before applying policy\n",
    "def gTrig(state):\n",
    "    return utils.gTrig_np(state, angle_dims).flatten()\n",
    "\n",
    "# during first n_rnd trials, apply randomized controls\n",
    "for i in range(n_rnd):\n",
    "    exp.new_episode()\n",
    "    apply_controller(env, randpol, H,\n",
    "                     preprocess=gTrig,\n",
    "                     callback=step_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PILCO loop\n",
    "rollout_fn = None\n",
    "fig, axarr = None, None\n",
    "for i in range(n_opt):\n",
    "    total_exp = sum([len(st) for st in exp.states])\n",
    "    msg = '==== Iteration [%d], experience: [%d steps] ===='\n",
    "    utils.print_with_stamp(msg % (i+1, total_exp))\n",
    "\n",
    "    # train dynamics model\n",
    "    train_dynamics(dyn, exp, angle_dims=angle_dims)\n",
    "\n",
    "    # initial state distribution\n",
    "    x0 = np.array([st[0] for st in exp.states])\n",
    "    m0 = x0.mean(0)\n",
    "    S0 = np.cov(x0, rowvar=False, ddof=1) +\\\n",
    "        1e-4*np.eye(x0.shape[1]) if len(x0) > 2 else p0.cov\n",
    "\n",
    "    if fig is not None:\n",
    "        # plot rollout\n",
    "        fig, axarr = plot_rollout(\n",
    "            rollout_fn, m0, S0, H, gamma, fig=fig, axarr=axarr)\n",
    "\n",
    "    # train policy\n",
    "    if polopt.loss_fn is None or dyn.should_recompile:\n",
    "        loss_kwargs = {}\n",
    "        obj_kwargs = {}\n",
    "        extra_inps = []\n",
    "        if use_bnn_dyn:\n",
    "            # init learning rate parameter\n",
    "            lr = theano.tensor.scalar('lr')\n",
    "            extra_inps += [lr]\n",
    "\n",
    "            # parameters for building loss function\n",
    "            loss_kwargs['n_samples'] = n_samples\n",
    "            loss_kwargs['resample_particles'] = False\n",
    "            obj_kwargs['learning_rate'] = lr\n",
    "            obj_kwargs['clip'] = 1.0\n",
    "            obj_kwargs['polyak_averaging'] = polyak_averaging\n",
    "            learner = mc_pilco\n",
    "        else:\n",
    "            learner = pilco\n",
    "\n",
    "        # build loss function\n",
    "        loss, inps, updts = learner.get_loss(\n",
    "            pol, dyn, cost, D, angle_dims, **loss_kwargs)\n",
    "        inps += extra_inps\n",
    "\n",
    "        # set objective of policy optimizer\n",
    "        polopt.set_objective(loss, pol.get_params(symbolic=True),\n",
    "                             inps, updts, **obj_kwargs)\n",
    "\n",
    "        # build rollout function for plotting\n",
    "        if rollout_fn is None:\n",
    "            loss_kwargs['resample_particles'] = False\n",
    "            rollout_fn = learner.build_rollout(\n",
    "                pol, dyn, cost, D, angle_dims, **loss_kwargs)\n",
    "\n",
    "    polopt_args = [m0, S0, H, gamma]\n",
    "    if use_bnn_dyn:\n",
    "        polopt_args.append(learning_rate)\n",
    "    polopt.minimize(*polopt_args,\n",
    "                    callback=polopt_cb,\n",
    "                    return_best=True)\n",
    "\n",
    "    # apply controller\n",
    "    exp.new_episode(policy_params=pol.get_params())\n",
    "    apply_controller(env, pol, H,\n",
    "                     preprocess=gTrig, callback=step_cb)\n",
    "\n",
    "    # plot rollout\n",
    "    fig, axarr = plot_rollout(\n",
    "        rollout_fn, m0, S0, H, gamma, fig=fig, axarr=axarr)\n",
    "input('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
